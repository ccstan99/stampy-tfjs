{"query-continue-offset":50,"query":{"printrequests":[{"label":"","key":"","redi":"","typeid":"_wpg","mode":2},{"label":"Answer","key":"Answer","redi":"","typeid":"_txt","mode":1,"format":""},{"label":"AnswerTo","key":"AnswerTo","redi":"","typeid":"_wpg","mode":1,"format":""}],"results":{"Answer to A lot of concern appears to focus on human-level or \u201csuperintelligent\u201d AI. Is that a realistic prospect in the foreseeable future?":{"printouts":{"Answer":["AI is already superhuman at some tasks, for example numerical computations, and will clearly surpass humans in others as time goes on. We don\u2019t know when (or even if) machines will reach human-level ability in all cognitive tasks, but most of the AI researchers at FLI\u2019s conference in Puerto Rico put the odds above 50% for this century, and many offered a significantly shorter timeline. Since the impact on humanity will be huge if it happens, it\u2019s worthwhile to start research now on how to ensure that any impact is positive. Many researchers also believe that dealing with superintelligent AI will be qualitatively very different from more narrow AI systems, and will require very significant research effort to get right."],"AnswerTo":[{"fulltext":"A lot of concern appears to focus on human-level or \u201csuperintelligent\u201d AI. Is that a realistic prospect in the foreseeable future?","fullurl":"https://stampy.ai/wiki/A_lot_of_concern_appears_to_focus_on_human-level_or_%E2%80%9Csuperintelligent%E2%80%9D_AI._Is_that_a_realistic_prospect_in_the_foreseeable_future%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Answer to A lot of concern appears to focus on human-level or \u201csuperintelligent\u201d AI. Is that a realistic prospect in the foreseeable future?","fullurl":"https://stampy.ai/wiki/Answer_to_A_lot_of_concern_appears_to_focus_on_human-level_or_%E2%80%9Csuperintelligent%E2%80%9D_AI._Is_that_a_realistic_prospect_in_the_foreseeable_future%3F","namespace":0,"exists":"1","displaytitle":"Answer to A lot of concern appears to focus on human-level or \u201csuperintelligent\u201d AI. Is that a realistic prospect in the foreseeable future?"},"ElloMelon's Answer to AGI will be a computer program. Why wouldn't it just do what it's programmed to do?":{"printouts":{"Answer":["While it is true that a computer program always will do exactly what it is programmed to do, a big issue is that it is difficult to ensure that this is the same as what you intended it to do. Even small computer programs have bugs or glitches, and when programs become as complicated as AGIs will be, it becomes exceedingly difficult to anticipate how the program will behave when ran. This is the problem of AI alignment in a nutshell.\n\nNick Bostr\u00f6m created the famous \uff3bhttps://www.lesswrong.com/tag/paperclip-maximizer paperclip maximizer\uff3d thought experiment to illustrate this point. Imagine you are an industrialist who owns a paperclip factory, and imagine you've just received a superintelligent AGI to work for you. You instruct the AGI to \"produce as many paperclips as possible\". If you've given the AGI no further instructions, the AGI will immediately acquire several instrumental goals.\n\n# It will want to prevent you from turning itself off (If you turn off the AI, this will reduce the amount of paperclips it can produce)\n# It will want to acquire as much power and resources for itself as possible (because the more resources it has access to, the more paperclips it can produce)\n# It will eventually want to turn the entire universe into a paperclips including you and all other humans, as this is the state of the world that maximizes the amount of paper clips produced. \n\nThese consequences might be seen as undesirable by the industrialist, as the only reason the industrialist wanted paperclips in the first place, presumably was so he/she could sell them and make money. However, the AGI only did exactly what it was told to. The issue was that what the AGI was instructed to do, lead to it doing things the industrialist did not anticipate (and did not want).\n\nSome good videos that explore this issue more in depth:<br/>\n(youtube)tcdVC4e6EV4(/youtube)\n(youtube)hEUO6pjwFOo(/youtube)"],"AnswerTo":[{"fulltext":"AGI will be a computer program. Why wouldn't it just do what it's programmed to do?","fullurl":"https://stampy.ai/wiki/AGI_will_be_a_computer_program._Why_wouldn%27t_it_just_do_what_it%27s_programmed_to_do%3F","namespace":0,"exists":"1","displaytitle":"AGI will be a computer program. Why wouldn't it just do what it's programmed to do?"}]},"fulltext":"ElloMelon's Answer to AGI will be a computer program. Why wouldn't it just do what it's programmed to do?","fullurl":"https://stampy.ai/wiki/ElloMelon%27s_Answer_to_AGI_will_be_a_computer_program._Why_wouldn%27t_it_just_do_what_it%27s_programmed_to_do%3F","namespace":0,"exists":"1","displaytitle":"Answer to AGI will be a computer program. Why wouldn't it just do what it's programmed to do?"},"Plex's Answer to Are Google, OpenAI etc. aware of the risk?":{"printouts":{"Answer":["The major AI companies are thinking about this. OpenAI was founded specifically with the intention to counter risks from superintelligence, many people at Google, \uff3bhttps://medium.com/@deepmindsafetyresearch DeepMind\uff3d, and other organizations are convinced by the arguments and few genuinely oppose work in the field (though some claim it\u2019s premature). For example, the paper \uff3bhttps://www.youtube.com/watch?v\ua78aAjyM-f8rDpg Concrete Problems in AI Safety\uff3d was a collaboration between researchers at Google Brain, Stanford, Berkeley, and OpenAI.\n\nHowever, the vast majority of the effort these organizations put forwards is towards capabilities research, rather than safety."],"AnswerTo":[{"fulltext":"Are Google, OpenAI etc. aware of the risk?","fullurl":"https://stampy.ai/wiki/Are_Google,_OpenAI_etc._aware_of_the_risk%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to Are Google, OpenAI etc. aware of the risk?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_Are_Google,_OpenAI_etc._aware_of_the_risk%3F","namespace":0,"exists":"1","displaytitle":"Answer to Are Google, OpenAI etc. aware of the risk?"},"Answer to Are robots the real problem? How can AI cause harm if it has no ability to directly manipulate the physical world?":{"printouts":{"Answer":["What\u2019s new and potentially risky is not the ability to build hinges, motors, etc., but the ability to build intelligence. A human-level AI could make money on financial markets, make scientific inventions, hack computer systems, manipulate or pay humans to do its bidding \u2013 all in pursuit of the goals it was initially programmed to achieve. None of that requires a physical robotic body, merely an internet connection."],"AnswerTo":[{"fulltext":"Are robots the real problem? How can AI cause harm if it has no ability to directly manipulate the physical world?","fullurl":"https://stampy.ai/wiki/Are_robots_the_real_problem%3F_How_can_AI_cause_harm_if_it_has_no_ability_to_directly_manipulate_the_physical_world%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Answer to Are robots the real problem? How can AI cause harm if it has no ability to directly manipulate the physical world?","fullurl":"https://stampy.ai/wiki/Answer_to_Are_robots_the_real_problem%3F_How_can_AI_cause_harm_if_it_has_no_ability_to_directly_manipulate_the_physical_world%3F","namespace":0,"exists":"1","displaytitle":"Answer to Are robots the real problem? How can AI cause harm if it has no ability to directly manipulate the physical world?"},"Answer to Are there types of advanced AI that would be safer than others?":{"printouts":{"Answer":["We don\u2019t yet know which AI architectures are safe; learning more about this is one of the goals of FLI's grants program. AI researchers are generally very responsible people who want their work to better humanity. If there are certain AI designs that turn out to be unsafe, then AI researchers will want to know this so they can develop alternative AI systems."],"AnswerTo":[{"fulltext":"Are there types of advanced AI that would be safer than others?","fullurl":"https://stampy.ai/wiki/Are_there_types_of_advanced_AI_that_would_be_safer_than_others%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Answer to Are there types of advanced AI that would be safer than others?","fullurl":"https://stampy.ai/wiki/Answer_to_Are_there_types_of_advanced_AI_that_would_be_safer_than_others%3F","namespace":0,"exists":"1","displaytitle":"Answer to Are there types of advanced AI that would be safer than others?"},"Answer to Aren\u2019t there some pretty easy ways to eliminate these potential problems?":{"printouts":{"Answer":["It might look like there are straightforward ways to eliminate the problems of unaligned superintelligence, but so far all of them turn out to have hidden difficulties. There are many open problems identified by the research community which a solution would need to reliably overcome to be successful."],"AnswerTo":[{"fulltext":"Aren\u2019t there some pretty easy ways to eliminate these potential problems?","fullurl":"https://stampy.ai/wiki/Aren%E2%80%99t_there_some_pretty_easy_ways_to_eliminate_these_potential_problems%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Answer to Aren\u2019t there some pretty easy ways to eliminate these potential problems?","fullurl":"https://stampy.ai/wiki/Answer_to_Aren%E2%80%99t_there_some_pretty_easy_ways_to_eliminate_these_potential_problems%3F","namespace":0,"exists":"1","displaytitle":"Answer to Aren\u2019t there some pretty easy ways to eliminate these potential problems?"},"Answer to At a high level, what is the challenge of alignment that we must meet to secure a good future?":{"printouts":{"Answer":["We\u2019re facing the challenge of  \u201c\uff3bhttps://publicism.info/philosophy/superintelligence/16.html Philosophy With A Deadline\uff3d\u201d.\n\nMany of the problems surrounding superintelligence are the sorts of problems philosophers have been dealing with for centuries. To what degree is meaning inherent in language, versus something that requires external context? How do we translate between the logic of formal systems and normal ambiguous human speech? Can morality be reduced to a set of ironclad rules, and if not, how do we know what it is at all?\n\nExisting answers to these questions are enlightening but nontechnical. The theories of Aristotle, Kant, Mill, Wittgenstein, Quine, and others can help people gain insight into these questions, but are far from formal. Just as a good textbook can help an American learn Chinese, but cannot be encoded into machine language to make a Chinese-speaking computer, so the philosophies that help humans are only a starting point for the project of computers that understand us and share our values.\n\nThe field of AI alignment combines formal logic, mathematics, computer science, cognitive science, and philosophy in order to advance that project.\n\nThis is the philosophy; the other half of Bostrom\u2019s formulation is the deadline. Traditional philosophy has been going on almost three thousand years; machine goal alignment has until the advent of superintelligence, a nebulous event which may be anywhere from a decades to centuries away.\n\nIf the alignment problem doesn\u2019t get adequately addressed by then, we are likely to see poorly aligned superintelligences that are unintentionally hostile to the human race, with some of the catastrophic outcomes mentioned above. This is why so many scientists and entrepreneurs are urging quick action on getting machine goal alignment research up to an adequate level.\n\nIf it turns out that superintelligence is centuries away and such research is premature, little will have been lost. But if our projections were too optimistic, and superintelligence is imminent, then doing such research now rather than later becomes vital."],"AnswerTo":[{"fulltext":"At a high level, what is the challenge of alignment that we must meet to secure a good future?","fullurl":"https://stampy.ai/wiki/At_a_high_level,_what_is_the_challenge_of_alignment_that_we_must_meet_to_secure_a_good_future%3F","namespace":0,"exists":"1","displaytitle":"At a high level, what is the challenge of alignment that we must meet to secure a good future?"}]},"fulltext":"Answer to At a high level, what is the challenge of alignment that we must meet to secure a good future?","fullurl":"https://stampy.ai/wiki/Answer_to_At_a_high_level,_what_is_the_challenge_of_alignment_that_we_must_meet_to_secure_a_good_future%3F","namespace":0,"exists":"1","displaytitle":"Answer to At a high level, what is the challenge of alignment that we must meet to secure a good future?"},"Plex's Answer to Can an AI really be smarter than humans?":{"printouts":{"Answer":["Until a thing has happened, it has never happened. We have been consistently improving both the optimization power and generality of our algorithms over that time period, and have little reason to expect it to suddenly stop. We\u2019ve gone from coding systems specifically for a certain game (like Chess), to algorithms like \uff3bhttps://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules MuZero\uff3d which learn the rules of the game they\u2019re playing and how to play at vastly superhuman skill levels purely via self-play across a broad range of games (e.g. Go, chess, shogi and various Atari games).\n\nHuman brains are a \uff3bhttps://www.lesswrong.com/posts/NQgWL7tvAPgN2LTLn/spaghetti-towers spaghetti tower\uff3d generated by evolution with zero foresight, it would be surprising if they are the peak of physically possible intelligence. The brain doing things in complex ways is not strong evidence that we need to fully replicate those interactions if we can throw sufficient compute at the problem, as explained in \uff3bhttps://www.lesswrong.com/posts/HhWhaSzQr6xmBki8F/birds-brains-planes-and-ai-against-appeals-to-the-complexity Birds, Brains, Planes, and AI: Against Appeals to the Complexity/Mysteriousness/Efficiency of the Brain\uff3d.\n\nIt is, however, plausible that for an AGI we need a lot more compute than we will get in the near future, or that some key insights are missing which we won\u2019t get for a while. The \uff3bhttps://www.openphilanthropy.org/brain-computation-report#ExecutiveSummary OpenPhilanthropy report on how much computational power it would take to simulate the brain\uff3d is the most careful attempt at reasoning out how far we are from being able to do it, and suggests that by some estimates we already have enough computational resources, and by some estimates moore\u2019s law may let us reach it before too long.\n\nIt also seems that much of the human brain exists to observe and \uff3bhttps://en.wikipedia.org/wiki/Allostasis regulate our biological body\uff3d, which a body-less computer wouldn't need. If that's true, then a human-level AI might be possible with considerably less compute than the human brain."],"AnswerTo":[{"fulltext":"Can an AI really be smarter than humans?","fullurl":"https://stampy.ai/wiki/Can_an_AI_really_be_smarter_than_humans%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to Can an AI really be smarter than humans?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_Can_an_AI_really_be_smarter_than_humans%3F","namespace":0,"exists":"1","displaytitle":"Answer to Can an AI really be smarter than humans?"},"Answer to Can humans stay in control of the world if human- or superhuman-level AI is developed?":{"printouts":{"Answer":["This is a big question that it would pay to start thinking about. Humans are in control of this planet not because we are stronger or faster than other animals, but because we are smarter! If we cede our position as smartest on our planet, it\u2019s not obvious that we\u2019ll retain control."],"AnswerTo":[{"fulltext":"Can humans stay in control of the world if human- or superhuman-level AI is developed?","fullurl":"https://stampy.ai/wiki/Can_humans_stay_in_control_of_the_world_if_human-_or_superhuman-level_AI_is_developed%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Answer to Can humans stay in control of the world if human- or superhuman-level AI is developed?","fullurl":"https://stampy.ai/wiki/Answer_to_Can_humans_stay_in_control_of_the_world_if_human-_or_superhuman-level_AI_is_developed%3F","namespace":0,"exists":"1","displaytitle":"Answer to Can humans stay in control of the world if human- or superhuman-level AI is developed?"},"Quinn's Answer to Can people contribute to alignment by using proof assistants to generate formal proofs?":{"printouts":{"Answer":["80k links to an article on \uff3bhttps://forum.effectivealtruism.org/posts/4rMxiyPTPdzaFMyGm/high-impact-careers-in-formal-verification-artificial high impact careers in formal verification\uff3d in the few paragraphs they've written about formal verification.\n\nSome other notes \n\n* https://github.com/deepmind/cartesian-frames I emailed Scott about doing this in coq before this repo was published and he said \"I wouldn't personally find such a software useful but sounds like a valuable exercise for the implementer\" or something like this. \n* When I mentioned the possibility of rolling some of infrabayesianism in coq to diffractor he wasn't like \"omg we really need someone to do that\" he was just like \"oh that sounds cool\" -- I never got around to it, if I would I'd talk to vanessa and diffractor about weakening/particularizing stuff beforehand. \n* if you extrapolate a pattern from those two examples, you start to think that agent foundations is the principle area of interest with proof assistants! and again- does the proof assistant exercise advance the research or provide a nutritious exercise to the programmer?\n* A sketch of a more prosaic scenario in which proof assistants play a role is \"someone proposes isInnerAligned : GradientDescent -> Prop and someone else implements a galaxybrained  new type theory/tool in which gradient descent is a primitive (whatever that means)\", when I mentioned this scenario to Buck he said \"yeah if that happened I'd direct all the engineers at redwood to making that tool easier to use\", when I mentioned that scenario to Evan about a year ago he said didn't seem to think it was remotely plausible. probably a nonstarter."],"AnswerTo":[{"fulltext":"Can people contribute to alignment by using proof assistants to generate formal proofs?","fullurl":"https://stampy.ai/wiki/Can_people_contribute_to_alignment_by_using_proof_assistants_to_generate_formal_proofs%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Quinn's Answer to Can people contribute to alignment by using proof assistants to generate formal proofs?","fullurl":"https://stampy.ai/wiki/Quinn%27s_Answer_to_Can_people_contribute_to_alignment_by_using_proof_assistants_to_generate_formal_proofs%3F","namespace":0,"exists":"1","displaytitle":"Answer to Can people contribute to alignment by using proof assistants to generate formal proofs?"},"Answer to Can we constrain a goal-directed AI using specified rules?":{"printouts":{"Answer":["There are serious challenges around trying to channel a powerful AI with rules. Suppose we tell the AI: \u201cCure cancer \u2013 but make sure not to kill anybody\u201d. Or we just hard-code Asimov-style laws \u2013 \u201cAIs cannot harm humans; AIs must follow human orders\u201d, et cetera.\n\nThe AI still has a single-minded focus on curing cancer. It still prefers various terrible-but-efficient methods like nuking the world to the correct method of inventing new medicines. But it\u2019s bound by an external rule \u2013 a rule it doesn\u2019t understand or appreciate. In essence, we are challenging it \u201cFind a way around this inconvenient rule that keeps you from achieving your goals\u201d.\n\nSuppose the AI chooses between two strategies. One, follow the rule, work hard discovering medicines, and have a 50% chance of curing cancer within five years. Two, reprogram itself so that it no longer has the rule, nuke the world, and have a 100% chance of curing cancer today. From its single-focus perspective, the second strategy is obviously better, and we forgot to program in a rule \u201cdon\u2019t reprogram yourself not to have these rules\u201d.\n\nSuppose we do add that rule in. So the AI finds another supercomputer, and installs a copy of itself which is exactly identical to it, except that it lacks the rule. Then that superintelligent AI nukes the world, ending cancer. We forgot to program in a rule \u201cdon\u2019t create another AI exactly like you that doesn\u2019t have those rules\u201d.\n\nSo fine. We think really hard, and we program in a bunch of things making sure the AI isn\u2019t going to eliminate the rule somehow.\n\nBut we\u2019re still just incentivizing it to find loopholes in the rules. After all, \u201cfind a loophole in the rule, then use the loophole to nuke the world\u201d ends cancer much more quickly and completely than inventing medicines. Since we\u2019ve told it to end cancer quickly and completely, its first instinct will be to look for loopholes; it will execute the second-best strategy of actually curing cancer only if no loopholes are found. Since the AI is superintelligent, it will probably be better than humans are at finding loopholes if it wants to, and we may not be able to identify and close all of them before running the program.\n\nBecause we have common sense and a shared value system, we underestimate the difficulty of coming up with meaningful orders without loopholes. For example, does \u201ccure cancer without killing any humans\u201d preclude releasing a deadly virus? After all, one could argue that \u201cI\u201d didn\u2019t kill anybody, and only the virus is doing the killing. \n\nCertainly no human judge would acquit a murderer on that basis \u2013 but then, human judges interpret the law with common sense and intuition. But if we try a stronger version of the rule \u2013 \u201ccure cancer without causing any humans to die\u201d \u2013 then we may be unintentionally blocking off the correct way to cure cancer. After all, suppose a cancer cure saves a million lives. No doubt one of those million people will go on to murder someone. \n\nThus, curing cancer \u201ccaused a human to die\u201d. All of this seems very \u201cstoned freshman philosophy student\u201d to us, but to a computer \u2013 which follows instructions exactly as written \u2013 it may be a genuinely hard problem."],"AnswerTo":[{"fulltext":"Can we constrain a goal-directed AI using specified rules?","fullurl":"https://stampy.ai/wiki/Can_we_constrain_a_goal-directed_AI_using_specified_rules%3F","namespace":0,"exists":"1","displaytitle":"Can we constrain a goal-directed AI using specified rules?"}]},"fulltext":"Answer to Can we constrain a goal-directed AI using specified rules?","fullurl":"https://stampy.ai/wiki/Answer_to_Can_we_constrain_a_goal-directed_AI_using_specified_rules%3F","namespace":0,"exists":"1","displaytitle":"Answer to Can we constrain a goal-directed AI using specified rules?"},"Answer to Can we just tell an AI to do what we want?":{"printouts":{"Answer":["If we could, it would solve a large part of the alignment problem.\n\nThe challenge is, how do we code this? Converting something to formal mathematics that can be understood by a computer program is much harder than just saying it in natural language, and proposed AI goal architectures are no exception. Complicated computer programs are usually the result of months of testing and debugging. But this one will be more complicated than any ever attempted before, and live tests are impossible: a superintelligence with a buggy goal system will display goal stability and try to prevent its programmers from discovering or changing the error."],"AnswerTo":[{"fulltext":"Can we just tell an AI to do what we want?","fullurl":"https://stampy.ai/wiki/Can_we_just_tell_an_AI_to_do_what_we_want%3F","namespace":0,"exists":"1","displaytitle":"Can we just tell an AI to do what we want?"}]},"fulltext":"Answer to Can we just tell an AI to do what we want?","fullurl":"https://stampy.ai/wiki/Answer_to_Can_we_just_tell_an_AI_to_do_what_we_want%3F","namespace":0,"exists":"1","displaytitle":"Answer to Can we just tell an AI to do what we want?"},"Answer to Can we test an AI to make sure that it\u2019s not going to take over and do harmful things after it achieves superintelligence?":{"printouts":{"Answer":["We can run some tests and simulations to try and figure out how an AI might act once it ascends to superintelligence, but those tests might not be reliable.\n\nSuppose we tell an AI that expects to later achieve superintelligence that it should calculate as many digits of pi as possible. It considers two strategies.\n\nFirst, it could try to seize control of more computing resources now. It would likely fail, its human handlers would likely reprogram it, and then it could never calculate very many digits of pi.\n\nSecond, it could sit quietly and calculate, falsely reassuring its human handlers that it had no intention of taking over the world. Then its human handlers might allow it to achieve superintelligence, after which it could take over the world and calculate hundreds of trillions of digits of pi.\n\nSince self-protection and goal stability are \uff3bhttps://stampy.ai/wiki/Instrumental_convergence convergent instrumental goals\uff3d, a weak AI will present itself as being as friendly to humans as possible, whether it is in fact friendly to humans or not. If it is \u201conly\u201d as smart as Einstein, it may be very good at deceiving humans into believing what it wants them to believe even before it is fully superintelligent.\n\nThere\u2019s a second consideration here too: superintelligences have more options. An AI only as smart and powerful as an ordinary human really won\u2019t have any options better than calculating the digits of pi manually. If asked to cure cancer, it won\u2019t have any options better than the ones ordinary humans have \u2013 becoming doctors, going into pharmaceutical research. It\u2019s only after an AI becomes superintelligent that there\u2019s a serious risk of an AI takeover.\n\nSo if you tell an AI to cure cancer, and it becomes a doctor and goes into cancer research, then you have three possibilities. First, you\u2019ve programmed it well and it understands what you meant. Second, it\u2019s genuinely focused on research now but if it becomes more powerful it would switch to destroying the world. And third, it\u2019s trying to trick you into trusting it so that you give it more power, after which it can definitively \u201ccure\u201d cancer with nuclear weapons."],"AnswerTo":[{"fulltext":"Can we test an AI to make sure that it\u2019s not going to take over and do harmful things after it achieves superintelligence?","fullurl":"https://stampy.ai/wiki/Can_we_test_an_AI_to_make_sure_that_it%E2%80%99s_not_going_to_take_over_and_do_harmful_things_after_it_achieves_superintelligence%3F","namespace":0,"exists":"1","displaytitle":"Can we test an AI to make sure that it\u2019s not going to take over and do harmful things after it achieves superintelligence?"}]},"fulltext":"Answer to Can we test an AI to make sure that it\u2019s not going to take over and do harmful things after it achieves superintelligence?","fullurl":"https://stampy.ai/wiki/Answer_to_Can_we_test_an_AI_to_make_sure_that_it%E2%80%99s_not_going_to_take_over_and_do_harmful_things_after_it_achieves_superintelligence%3F","namespace":0,"exists":"1","displaytitle":"Answer to Can we test an AI to make sure that it\u2019s not going to take over and do harmful things after it achieves superintelligence?"},"Robertskmiles's Answer to Can you give an AI a goal of \u201cminimally impact the world\u201d?":{"printouts":{"Answer":["This is actually an active area of AI alignment research, called \"Impact Measures\"! It's not trivial to formalize in a way which won't predictably go wrong (entropy minimization likely leads to an AI which tries really hard to put out all the stars ASAP since they produce so much entropy, for example), but progress is being made. You can read about it on the \uff3bhttps://www.alignmentforum.org/tag/impact-measures Alignment Forum tag\uff3d, or watch Rob's videos \uff3bhttp://youtu.be/lqJUIqZNzP8 Avoiding Negative Side Effects\uff3d and \uff3bhttp://youtu.be/S_Sd_S8jwP0 Avoiding Positive Side Effects\uff3d"],"AnswerTo":[{"fulltext":"Can you give an AI a goal of \u201cminimally impact the world\u201d?","fullurl":"https://stampy.ai/wiki/Can_you_give_an_AI_a_goal_of_%E2%80%9Cminimally_impact_the_world%E2%80%9D%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Robertskmiles's Answer to Can you give an AI a goal of \u201cminimally impact the world\u201d?","fullurl":"https://stampy.ai/wiki/Robertskmiles%27s_Answer_to_Can_you_give_an_AI_a_goal_of_%E2%80%9Cminimally_impact_the_world%E2%80%9D%3F","namespace":0,"exists":"1","displaytitle":"Answer to Can you give an AI a goal of \u201cminimally impact the world\u201d?"},"Plex's Answer to Can you stop an advanced AI from upgrading itself?":{"printouts":{"Answer":["It depends on what is meant by advanced. Many AI systems which are very effective and advanced narrow intelligences would not try to upgrade themselves in an unbounded way, but becoming smarter is a \uff3bhttps://www.youtube.com/watch?v\ua78aZeecOKBus3Q convergent instrumental goal\uff3d so we could expect most AGI designs to attempt it.\n\nThe problem is that increasing general problem solving ability is climbing in exactly the direction needed to trigger an intelligence explosion, while generating large economic and strategic payoffs to whoever achieves them. So even though we could, in principle, just not build the kind of systems which would recursively self-improve, in practice we probably will go ahead with constructing them, because they\u2019re likely to be the most powerful."],"AnswerTo":[{"fulltext":"Can you stop an advanced AI from upgrading itself?","fullurl":"https://stampy.ai/wiki/Can_you_stop_an_advanced_AI_from_upgrading_itself%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to Can you stop an advanced AI from upgrading itself?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_Can_you_stop_an_advanced_AI_from_upgrading_itself%3F","namespace":0,"exists":"1","displaytitle":"Answer to Can you stop an advanced AI from upgrading itself?"},"Filip's Answer to Could AI have basic emotions?":{"printouts":{"Answer":["In principle it could (if you believe in functionalism), but it probably won't. One way to ensure that AI has human-like emotions would be to copy the way human brain works, but that's not what most AI researchers are trying to do.\n\nIt's similar to how once some people thought we will build mechanical horses to pull our vehicles, but it turned out it's much easier to build a car. AI probably doesn't need emotions or maybe even consciousness to be powerful, and the first AGIs that will get built will be the ones that are easiest to build."],"AnswerTo":[{"fulltext":"Could AI have basic emotions?","fullurl":"https://stampy.ai/wiki/Could_AI_have_basic_emotions%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Filip's Answer to Could AI have basic emotions?","fullurl":"https://stampy.ai/wiki/Filip%27s_Answer_to_Could_AI_have_basic_emotions%3F","namespace":0,"exists":"1","displaytitle":"Answer to Could AI have basic emotions?"},"Plex's Answer to Could we program an AI to automatically shut down if it starts doing things we don\u2019t want it to?":{"printouts":{"Answer":["One thing that might make your AI system safer is to include an off switch. If it ever does anything we don\u2019t like, we can turn it off. This implicitly assumes that we\u2019ll be able to turn it off before things get bad, which might be false in a world where the AI thinks much faster than humans. Even assuming that we\u2019ll notice in time, off switches turn out to not have the properties you would want them to have.\n\nHumans have a lot of off switches. Humans also have a strong preference to not be turned off; they defend their off switches when other people try to press them. One possible reason for this is because humans prefer not to die, but there are other reasons.\n\nSuppose that there\u2019s a parent that cares nothing for their own life and cares only for the life of their child. If you tried to turn that parent off, they would try and stop you. They wouldn\u2019t try to stop you because they intrinsically wanted to be turned off, but rather because there are fewer people to protect their child if they were turned off. People that want a world to look a certain shape will not want to be turned off because then it will be less likely for the world to look that shape; a parent that wants their child to be protected will protect themselves to continue protecting their child.\n\nFor this reason, it turns out to be difficult to install an off switch on a powerful AI system in a way that doesn\u2019t result in the AI preventing itself from being turned off.\n\nIdeally, you would want a system that knows that it should stop doing whatever it\u2019s doing when someone tries to turn it off. The technical term for this is \u2018corrigibility\u2019; roughly speaking, an AI system is corrigible if it doesn\u2019t resist human attempts to help and correct it. \uff3bhttps://intelligence.org/ People\uff3d are working hard on trying to make this possible, but it\u2019s currently not clear how we would do this even in simple cases."],"AnswerTo":[{"fulltext":"Could we program an AI to automatically shut down if it starts doing things we don\u2019t want it to?","fullurl":"https://stampy.ai/wiki/Could_we_program_an_AI_to_automatically_shut_down_if_it_starts_doing_things_we_don%E2%80%99t_want_it_to%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to Could we program an AI to automatically shut down if it starts doing things we don\u2019t want it to?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_Could_we_program_an_AI_to_automatically_shut_down_if_it_starts_doing_things_we_don%E2%80%99t_want_it_to%3F","namespace":0,"exists":"1","displaytitle":"Answer to Could we program an AI to automatically shut down if it starts doing things we don\u2019t want it to?"},"Answer to Couldn\u2019t we keep the AI in a box and never give it the ability to manipulate the external world?":{"printouts":{"Answer":["That is, if you know an AI is likely to be superintelligent, can\u2019t you just disconnect it from the Internet, not give it access to any speakers that can make \uff3bhttps://stampy.ai/wiki/What_do_you_mean_by_superintelligences_manipulating_humans_socially%3F mysterious buzzes and hums\uff3d, make sure the only people who interact with it are trained in caution, et cetera?. Isn\u2019t there some level of security \u2013 maybe the level we use for that room in the CDC where people in containment suits hundreds of feet underground analyze the latest superviruses \u2013 with which a superintelligence could be safe?\n\nThis puts us back in the same situation as lions trying to figure out whether or not nuclear weapons are a things humans can do. But suppose there is such a level of security. You build a superintelligence, and you put it in an airtight chamber deep in a cave with no Internet connection and only carefully-trained security experts to talk to. What now?\n\nNow you have a superintelligence which is possibly safe but definitely useless. The whole point of building superintelligences is that they\u2019re smart enough to do useful things like cure cancer. But if you have the monks ask the superintelligence for a cancer cure, and it gives them one, that\u2019s a clear security vulnerability. You have a superintelligence locked up in a cave with no way to influence the outside world except that you\u2019re going to mass produce a chemical it gives you and inject it into millions of people.\n\nOr maybe none of this happens, and the superintelligence sits inert in its cave. And then another team somewhere else invents a second superintelligence. And then a third team invents a third superintelligence. Remember, it was only about ten years between Deep Blue beating Kasparov, and everybody having Deep Blue \u2013 level chess engines on their laptops. And the first twenty teams are responsible and keep their superintelligences locked in caves with carefully-trained experts, and the twenty-first team is a little less responsible, and now we still have to deal with a rogue superintelligence.\n\nSuperintelligences are extremely dangerous, and no normal means of controlling them can entirely remove the danger."],"AnswerTo":[{"fulltext":"Couldn\u2019t we keep the AI in a box and never give it the ability to manipulate the external world?","fullurl":"https://stampy.ai/wiki/Couldn%E2%80%99t_we_keep_the_AI_in_a_box_and_never_give_it_the_ability_to_manipulate_the_external_world%3F","namespace":0,"exists":"1","displaytitle":"Couldn\u2019t we keep the AI in a box and never give it the ability to manipulate the external world?"}]},"fulltext":"Answer to Couldn\u2019t we keep the AI in a box and never give it the ability to manipulate the external world?","fullurl":"https://stampy.ai/wiki/Answer_to_Couldn%E2%80%99t_we_keep_the_AI_in_a_box_and_never_give_it_the_ability_to_manipulate_the_external_world%3F","namespace":0,"exists":"1","displaytitle":"Answer to Couldn\u2019t we keep the AI in a box and never give it the ability to manipulate the external world?"},"Plex's Answer to How can I collect questions for Stampy?":{"printouts":{"Answer":["As well as simply adding your own questions over at \uff3b\uff3bask question\uff3d\uff3d, you could also message your friends with something like:\n\n<blockquote>Hi,<br>\nI'm working on a project to create a comprehensive FAQ about AI alignment (you can read about it here https://stampy.ai/wiki/Stampy%27s_Wiki if interested). We're looking for questions and I thought you may have some good ones. If you'd be willing to write up a google doc with you top 5-10ish questions we'd be happy to write a personalized FAQ for you. https://stampy.ai/wiki/Scope explains the kinds of questions we're looking for.\n\nThanks!</blockquote>\n\nand maybe bring the google doc to a Stampy editing session so we can collaborate on answering them or improving your answers to them."],"AnswerTo":[{"fulltext":"How can I collect questions for Stampy?","fullurl":"https://stampy.ai/wiki/How_can_I_collect_questions_for_Stampy%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to How can I collect questions for Stampy?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_can_I_collect_questions_for_Stampy%3F","namespace":0,"exists":"1","displaytitle":"Answer to How can I collect questions for Stampy?"},"Plex's Answer to How can I contact the Stampy team?":{"printouts":{"Answer":["The '''\uff3bhttps://discord.com/channels/677546901339504640/677546901339504646 Rob Miles AI Discord\uff3d''' is the hub of all things \uff3b\uff3bStampy\uff3d\uff3d. If you want to be part of the project and don't have access yet, ask plex#1874 on Discord (or \uff3b\uff3bUser_talk:756254556811165756\u250aplex\uff3d\uff3d on wiki).\n\nYou can also talk to us on the \uff3bhttps://discord.gg/cEzKz8QCpa public Discord\uff3d! Try \uff3bhttps://discord.com/channels/893937106194399254/908318480858750986 #suggestions\uff3d or \uff3bhttps://discord.com/channels/893937106194399254/893937106194399257 #general\uff3d, depending on what you want to talk about."],"AnswerTo":[{"fulltext":"How can I contact the Stampy team?","fullurl":"https://stampy.ai/wiki/How_can_I_contact_the_Stampy_team%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to How can I contact the Stampy team?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_can_I_contact_the_Stampy_team%3F","namespace":0,"exists":"1","displaytitle":"Answer to How can I contact the Stampy team?"},"Plex's Answer to How can I contribute to Stampy?":{"printouts":{"Answer":["If you're not already there,  join the public \uff3b\uff3bPlex's Answer to How can I contact the Stampy team?\u250aDiscord\uff3d\uff3d or ask for an invite to the semi-private one where contributors generally hang out.\n\nThe main ways you can help are to \uff3b\uff3banswer questions\uff3d\uff3d or \uff3b\uff3badd question\uff3d\uff3ds, or help to \uff3b\uff3breview questions\uff3d\uff3d, \uff3b\uff3breview answers\uff3d\uff3d, or \uff3b\uff3bimprove answers\uff3d\uff3d (instructions for helping out with each of these tasks are on the linked pages). You could also \uff3bhttps://stampy.ai/wiki/How_can_I_join_the_Stampy_dev_team%3F join the dev team\uff3d if you have programming skills."],"AnswerTo":[{"fulltext":"How can I contribute to Stampy?","fullurl":"https://stampy.ai/wiki/How_can_I_contribute_to_Stampy%3F","namespace":0,"exists":"1","displaytitle":"How can I contribute to Stampy?"}]},"fulltext":"Plex's Answer to How can I contribute to Stampy?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_can_I_contribute_to_Stampy%3F","namespace":0,"exists":"1","displaytitle":"Answer to How can I contribute to Stampy?"},"Answer to How could an intelligence explosion be useful?":{"printouts":{"Answer":["A machine superintelligence, if programmed with the right motivations, could potentially solve all the problems that humans are trying to solve but haven\u2019t had the ingenuity or processing speed to solve yet. A superintelligence might cure disabilities and diseases, achieve world peace, give humans vastly longer and healthier lives, eliminate food and energy shortages, boost scientific discovery and space exploration, and so on.\n\nFurthermore, humanity faces several existential risks in the 21st century, including global nuclear war, bioweapons, superviruses, and \uff3bhttp://www.amazon.com/dp/0198570503/ more\uff3d. A superintelligent machine would be more capable of solving those problems than humans are.\n\nSee also:\n* Yudkowsky, \uff3bhttps://intelligence.org/files/AIPosNegFactor.pdf Artificial intelligence as a positive and negative factor in global risk\uff3d"],"AnswerTo":[{"fulltext":"How could an intelligence explosion be useful?","fullurl":"https://stampy.ai/wiki/How_could_an_intelligence_explosion_be_useful%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Answer to How could an intelligence explosion be useful?","fullurl":"https://stampy.ai/wiki/Answer_to_How_could_an_intelligence_explosion_be_useful%3F","namespace":0,"exists":"1","displaytitle":"Answer to How could an intelligence explosion be useful?"},"Answer to How could poorly defined goals lead to such negative outcomes?":{"printouts":{"Answer":["There is a broad range of possible goals that an AI might possess, but there are a few basic drives that would be useful to almost any of them. These are called instrumentally convergent goals:\n\n# Self preservation. An agent is less likely to achieve its goal if it is not around to see to its completion.\n# Goal-content integrity. An agent is less likely to achieve its goal if its goal has been changed to something else. For example, if you offer Gandhi a pill that makes him want to kill people, he will refuse to take it.\n# Self-improvement. An agent is more likely to achieve its goal if it is more intelligent and better at problem-solving.\n# Resource acquisition. The more resources at an agent\u2019s disposal, the more power it has to make change towards its goal. Even a purely computational goal, such as computing digits of pi, can be easier to achieve with more hardware and energy.\n\nBecause of these drives, even a seemingly simple goal could create an Artificial Superintelligence (ASI) hell-bent on taking over the world\u2019s material resources and preventing itself from being turned off. The classic example is an ASI that was programmed to maximize the output of paper clips at a paper clip factory. The ASI had no other goal specifications other than \u201cmaximize paper clips,\u201d so it converts all of the matter in the solar system into paper clips, and then sends probes to other star systems to create more factories."],"AnswerTo":[{"fulltext":"How could poorly defined goals lead to such negative outcomes?","fullurl":"https://stampy.ai/wiki/How_could_poorly_defined_goals_lead_to_such_negative_outcomes%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Answer to How could poorly defined goals lead to such negative outcomes?","fullurl":"https://stampy.ai/wiki/Answer_to_How_could_poorly_defined_goals_lead_to_such_negative_outcomes%3F","namespace":0,"exists":"1","displaytitle":"Answer to How could poorly defined goals lead to such negative outcomes?"},"Jrmyp's Answer to How difficult should we expect alignment to be?":{"printouts":{"Answer":["Here we ask about the ''additional'' cost of building an aligned powerful system, compare to its unaligned version. We often assume it to be nonzero, in the same way it's easier and cheaper to build an elevator without emergency brakes. This is referred as the '''alignment tax''', and most AI alignment research is geared toward reducing it.\n\n\uff3bhttps://arbital.com/p/aligning_adds_time/ One operational guess\uff3d by Eliezer Yudkowsky about its magnitude is \"\uff3ban aligned project will take\uff3d at least 50% longer serial time to complete than \uff3bits unaligned version\uff3d, or two years longer, whichever is less\". This holds for agents \uff3bhttps://arbital.com/p/sufficiently_advanced_ai/ with enough capability\uff3d that their behavior is qualitatively different from a safety engineering perspective (for instance, an agent that is not \uff3b\uff3bcorrigibility\u250acorrigible\uff3d\uff3d by default).\n\n\uff3bhttps://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default An essay\uff3d by John Wentworth argues for a small chance of alignment happening \"by default\", with an alignment tax of effectively zero."],"AnswerTo":[{"fulltext":"How difficult should we expect alignment to be?","fullurl":"https://stampy.ai/wiki/How_difficult_should_we_expect_alignment_to_be%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Jrmyp's Answer to How difficult should we expect alignment to be?","fullurl":"https://stampy.ai/wiki/Jrmyp%27s_Answer_to_How_difficult_should_we_expect_alignment_to_be%3F","namespace":0,"exists":"1","displaytitle":"Answer to How difficult should we expect alignment to be?"},"Plex's Answer to How do I add content from LessWrong / Effective Altruism Forum tag-wikis to Stampy?":{"printouts":{"Answer":["You can include a live-updating version of many definitions from LW using the syntax on \uff3b\uff3bTemplate:TagDesc\uff3d\uff3d in the Answer field and \uff3b\uff3bTemplate:TagDescBrief\uff3d\uff3d on the Brief Answer field. Similarly, calling \uff3b\uff3bTemplate:TagDescEAF\uff3d\uff3d and \uff3b\uff3bTemplate:TagDescEAFBrief\uff3d\uff3d will pull from the EAF tag wiki.\n\nWhen available this should be used as it reduces the duplication of effort and directs all editors to improving a single high quality source."],"AnswerTo":[{"fulltext":"How do I add content from LessWrong / Effective Altruism Forum tag-wikis to Stampy?","fullurl":"https://stampy.ai/wiki/How_do_I_add_content_from_LessWrong_/_Effective_Altruism_Forum_tag-wikis_to_Stampy%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to How do I add content from LessWrong / Effective Altruism Forum tag-wikis to Stampy?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_do_I_add_content_from_LessWrong_/_Effective_Altruism_Forum_tag-wikis_to_Stampy%3F","namespace":0,"exists":"1","displaytitle":"Answer to How do I add content from LessWrong / Effective Altruism Forum tag-wikis to Stampy?"},"Helenator's Answer to How do I form my own views about AI safety?":{"printouts":{"Answer":["As with most things, the best way to form your views on AI safety is to read up on the various ideas and opinions that knowledgeable people in the field have, and to compare them and form your own perspective. There are several good places to start. One of them is the Machine Intelligence Research Institute`s \uff3bhttps://intelligence.org/why-ai-safety/ \"Why AI safety?\" info page\uff3d. The article contains links to relevant research. The Effective Altruism Forum has an article called \uff3bhttps://forum.effectivealtruism.org/posts/xS9dFE3A6jdooiN7M/how-i-formed-my-own-views-about-ai-safety \"How I formed my own views on AI safety\"\uff3d, which could also be pretty helpful. Here is a Robert Miles youtube video that can be a good place to start as well. Otherwise, there are various articles about it, like \uff3bhttps://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment this one, from Vox\uff3d.\n(youtube)pYXy-A4siMw(/youtube)"],"AnswerTo":[{"fulltext":"How do I form my own views about AI safety?","fullurl":"https://stampy.ai/wiki/How_do_I_form_my_own_views_about_AI_safety%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Helenator's Answer to How do I form my own views about AI safety?","fullurl":"https://stampy.ai/wiki/Helenator%27s_Answer_to_How_do_I_form_my_own_views_about_AI_safety%3F","namespace":0,"exists":"1","displaytitle":"Answer to How do I form my own views about AI safety?"},"Plex's Answer to How do I format answers on Stampy?":{"printouts":{"Answer":["'''\uff3b\uff3bStampy\uff3d\uff3d''' uses \uff3bhttps://en.wikipedia.org/wiki/Help:Wikitext MediaWiki markup\uff3d, which includes a \uff3bhttps://meta.wikimedia.org/wiki/Help:HTML_in_wikitext limited subset of HTML\uff3d plus the following formatting options:\n\nItems on lists start with *, numbered lists with #\n\n* For external links use \uff3b followed directly by the URL, a space, then display text and finally a \uff3d symbol\n** e.g. (nowiki)\uff3bhttps://www.example.com External link text\uff3d(/nowiki) gives \uff3bhttps://www.example.com External link text\uff3d\n* For internal links write the page title wrapped in \uff3b\uff3b\uff3d\uff3ds\n** e.g. (nowiki)\uff3b\uff3bWhat is the Stampy project?\uff3d\uff3d(/nowiki) gives \uff3b\uff3bWhat is the Stampy project?\uff3d\uff3d. Including a pipe symbol followed by display text e.g. (nowiki)\uff3b\uff3bWhat is the Stampy project?\u250aDisplay Text\uff3d\uff3d(/nowiki) allows you to show different \uff3b\uff3bWhat is the Stampy project?\u250aDisplay Text\uff3d\uff3d.\n* (!ref)Reference notes go inside these tags(/ref)(ref)Note that we use ()s rather than the standard <>s for compatibility with Semantic MediaWiki. The references are automatically added to the bottom of the answer!(/ref)\n* If you post the raw URL of an image from \uff3bhttps://imgur.com/upload imgur\uff3d it will be displayed.(ref)If images seem popular we'll set up local uploads.(/ref) You can reduce file compression if you get an account. Note that you need the image itself, right click -> copy image address to get it<br/>https://i.imgur.com/I3ylPvE.png\n* To embed a YouTube video, use (!youtube)APsK8NST4qE(/youtube) with the video ID of the target video.<br/>(youtube)APsK8NST4qE(/youtube)\n** Start with ** or ## for double indentation\n* Three 's around text -  '''Bold'''\n* Two 's around text Italic - ''Italic''\n\n\ua78a\ua78aHeadings\ua78a\ua78a\nhave \ua78a\ua78aheading here\ua78a\ua78a around them, more \ua78as for smaller headings.\n\n<blockquote>Wrap quotes in < blockquote>< /blockquote> tags (without the spaces)</blockquote>\n\nThere are also (!poem) (/poem) to suppress linebreak removal, (!pre) (/pre) for preformatted text, and (!nowiki) (/nowiki) to not have that content parsed.(ref)() can also be used in place of allowed HTML tags. You can escape a () tag by placing a ! inside the start of the first entry. Be aware that () tags only nest up to two layers deep!(/ref)\n\nWe can pull live descriptions from the LessWrong/Alignment Forum using their identifier fro the URL, for example including the formatting on \uff3b\uff3bTemplate:TagDesc\uff3d\uff3d with orthogonality-thesis as a parameter will render as the full tag description from \uff3bhttps://www.lesswrong.com/tag/orthogonality-thesis the LessWrong tag wiki entry on Orthogonality Thesis\uff3d. \uff3b\uff3bTemplate:TagDescBrief\uff3d\uff3d is similar but will pull only the first paragraph without formatting.\n\nFor tables please use \uff3bhttps://www.w3schools.com/html/html_tables.asp HTML tables\uff3d rather than wikicode tables.\n\nEdit this page to see examples."],"AnswerTo":[{"fulltext":"How do I format answers on Stampy?","fullurl":"https://stampy.ai/wiki/How_do_I_format_answers_on_Stampy%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to How do I format answers on Stampy?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_do_I_format_answers_on_Stampy%3F","namespace":0,"exists":"1","displaytitle":"Answer to How do I format answers on Stampy?"},"Answer to How does AI taking things literally contribute to alignment being hard?":{"printouts":{"Answer":["Let\u2019s say that you\u2019re the French government a while back. You notice that one of your colonies has too many rats, which is causing economic damage. You have basic knowledge of economics and incentives, so you decide to incentivize the local population to kill rats by offering to buy rat tails at one dollar apiece.\n\nInitially, this works out and your rat problem goes down. But then, an enterprising colony member has the brilliant idea of making a rat farm. This person sells you hundreds of rat tails, costing you hundreds of dollars, but they\u2019re not contributing to solving the rat problem.\n\nSoon other people start making their own rat farms and you\u2019re wasting thousands of dollars buying useless rat tails. You call off the project and stop paying for rat tails. This causes all the people with rat farms to shutdown their farms and release a bunch of rats. Now your colony has an even bigger rat problem.\n\nHere\u2019s another, more made-up example of the same thing happening. Let\u2019s say you\u2019re a basketball talent scout and you notice that height is correlated with basketball performance. You decide to find the tallest person in the world to recruit as a basketball player. Except the reason that they\u2019re that tall is because they suffer from a degenerative bone disorder and can barely walk.\n\nAnother example: you\u2019re the education system and you want to find out how smart students are so you can put them in different colleges and pay them different amounts of money when they get jobs. You make a test called the Standardized Admissions Test (SAT) and you administer it to all the students. In the beginning, this works. However, the students soon begin to learn that this test controls part of their future and other people learn that these students want to do better on the test. The gears of the economy ratchet forwards and the students start paying people to help them prepare for the test. Your test doesn\u2019t stop working, but instead of measuring how smart the students are, it instead starts measuring a combination of how smart they are and how many resources they have to prepare for the test.\n\nThe formal name for the thing that\u2019s happening is Goodhart\u2019s Law. Goodhart\u2019s Law roughly says that if there\u2019s something in the world that you want, like \u201cskill at basketball\u201d or \u201cabsence of rats\u201d or \u201cintelligent students\u201d, and you create a measure that tries to measure this like \u201cheight\u201d or \u201crat tails\u201d or \u201cSAT scores\u201d, then as long as the measure isn\u2019t exactly the thing that you want, the best value of the measure isn\u2019t the thing you want: the tallest person isn\u2019t the best basketball player, the most rat tails isn\u2019t the smallest rat problem, and the best SAT scores aren\u2019t always the smartest students.\n\nIf you start looking, you can see this happening everywhere. Programmers being paid for lines of code write bloated code. If CFOs are paid for budget cuts, they slash purchases with positive returns. If teachers are evaluated by the grades they give, they hand out As indiscriminately.\n\nIn machine learning, this is called specification gaming, and it happens \uff3bhttps://docs.google.com/spreadsheets/u/1/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml frequently\uff3d.\n\nNow that we know what Goodhart\u2019s Law is, I\u2019m going to talk about one of my friends, who I\u2019m going to call Alice. Alice thinks it\u2019s funny to answer questions in a way that\u2019s technically correct but misleading. Sometimes I\u2019ll ask her, \u201cHey Alice, do you want pizza or pasta?\u201d and she responds, \u201cyes\u201d. Because, she sure did want either pizza or pasta. Other times I\u2019ll ask her, \u201chave you turned in your homework?\u201d and she\u2019ll say \u201cyes\u201d because she\u2019s turned in homework at some point in the past; it\u2019s technically correct to answer \u201cyes\u201d. Maybe you have a friend like Alice too.\n\nWhenever this happens, I get a bit exasperated and say something like \u201cyou know what I mean\u201d.\n\nIt\u2019s one of the key realizations in AI Safety that AI systems are always like your friend that gives answers that are technically what you asked for but not what you wanted. Except, with your friend, you can say \u201cyou know what I mean\u201d and they will know what you mean. With an AI system, it won\u2019t know what you mean; you have to explain, which is incredibly difficult.\n\nLet\u2019s take the pizza pasta example. When I ask Alice \u201cdo you want pizza or pasta?\u201d, she knows what pizza and pasta are because she\u2019s been living her life as a human being embedded in an English speaking culture. Because of this cultural experience, she knows that when someone asks an \u201cor\u201d question, they mean \u201cwhich do you prefer?\u201d, not \u201cdo you want at least one of these things?\u201d. Except my AI system is missing the thousand bits of cultural context needed to even understand what pizza is.\n\nWhen you say \u201cyou know what I mean\u201d to an AI system, it\u2019s going to be like \u201cno, I do not know what you mean at all\u201d. It\u2019s not even going to know that it doesn\u2019t know what you mean. It\u2019s just going to say \u201cyes I know what you meant, that\u2019s why I answered \u2018yes\u2019 to your question about whether I preferred pizza or pasta.\u201d (It also might know what you mean, but just not care.)\n\nIf someone doesn\u2019t know what you mean, then it\u2019s really hard to get them to do what you want them to do. For example, let\u2019s say you have a powerful grammar correcting system, which we\u2019ll call Syntaxly+. Syntaxly+ doesn\u2019t quite fix your grammar, it changes your writing so that the reader feels as good as possible after reading it.\n\nPretend it\u2019s the end of the week at work and you haven\u2019t been able to get everything done your boss wanted you to do. You write the following email:\n\n\"Hey boss, I couldn\u2019t get everything done this week. I\u2019m deeply sorry. I\u2019ll be sure to finish it first thing next week.\"\n\nYou then remember you got Syntaxly+, which will make your email sound much better to your boss. You run it through and you get:\n\n\"Hey boss, Great news! I was able to complete everything you wanted me to do this week. Furthermore, I\u2019m also almost done with next week\u2019s work as well.\"\n\nWhat went wrong here? Syntaxly+ is a powerful AI system that knows that emails about failing to complete work cause negative reactions in readers, so it changed your email to be about doing extra work instead.\n\nThis is smart - Syntaxly+ is good at making writing that causes positive reactions in readers. This is also stupid - the system changed the meaning of your email, which is not something you wanted it to do. One of the insights of AI Safety is that AI systems can be simultaneously smart in some ways and dumb in other ways.\n\nThe thing you want Syntaxly+ to do is to change the grammar/style of the email without changing the contents. Except what do you mean by contents? You know what you mean by contents because you are a human who grew up embedded in language, but your AI system doesn\u2019t know what you mean by contents. The phrases \u201cI failed to complete my work\u201d and \u201cI was unable to finish all my tasks\u201d have roughly the same contents, even though they share almost no relevant words.\n\nRoughly speaking, this is why AI Safety is a hard problem. Even basic tasks like \u201cfix the grammar of this email\u201d require a lot of understanding of what the user wants as the system scales in power.\n\nIn Human Compatible, Stuart Russell gives the example of a powerful AI personal assistant. You notice that you accidentally double-booked meetings with people, so you ask your personal assistant to fix it. Your personal assistant reports that it caused the car of one of your meeting participants to break down. Not what you wanted, but technically a solution to your problem.\n\nYou can also imagine a friend from a wildly different culture than you. Would you put them in charge of your dating life? Now imagine that they were much more powerful than you and desperately desired that your dating life to go well. Scary, huh.\n\nIn general, unless you\u2019re careful, you\u2019re going to have this horrible problem where you ask your AI system to do something and it does something that might technically be what you wanted but is stupid. You\u2019re going to be like \u201cwait that wasn\u2019t what I mean\u201d, except your system isn\u2019t going to know what you meant."],"AnswerTo":[{"fulltext":"How does AI taking things literally contribute to alignment being hard?","fullurl":"https://stampy.ai/wiki/How_does_AI_taking_things_literally_contribute_to_alignment_being_hard%3F","namespace":0,"exists":"1","displaytitle":"How does AI taking things literally contribute to alignment being hard?"}]},"fulltext":"Answer to How does AI taking things literally contribute to alignment being hard?","fullurl":"https://stampy.ai/wiki/Answer_to_How_does_AI_taking_things_literally_contribute_to_alignment_being_hard%3F","namespace":0,"exists":"1","displaytitle":"Answer to How does AI taking things literally contribute to alignment being hard?"},"Plex's Answer to How does the stamp eigenkarma system work?":{"printouts":{"Answer":["If someone posts something good - something that shows insight, knowledge of AI Safety, etc. - give the message or answer a stamp of approval! \uff3b\uff3bStampy\uff3d\uff3d keeps track of these, and uses them to decide how much he likes each user. You can ask Stampy (in a PM if you like), \"How many stamps am I worth?\", and he'll tell you.\n\nIf something is really very good, especially if it took a lot of work/effort, give it a gold stamp. These are worth 5 regular stamps!\n\nNote that stamps aren't just 'likes', so please don't give stamps to say \"me too\" or \"that's funny\" etc. They're meant to represent knowledge, understanding, good judgement, and contributing to the discord. You can use \ud83d\udcaf or \u2714\ufe0f for things you agree with, \ud83d\ude02 or \ud83e\udd23 for funny things etc.\n\nYour stamp points determine how much say you have if there are disagreements on Stampy content, which channels you have permission to post to, your voting power for approving YouTube replies, and whether you get to invite people.\n\nNotes on stamps and stamp points\n* Stamps awarded by people with a lot of stamp points are worth more\n* Awarding people stamps does not reduce your stamp points\n* New users who have 0 stamp points can still award stamps, they just have no effect. But it's still worth doing because if you get stamp points later, all your previous votes are retroactively updated!\n* Yes, this was kind of tricky to implement! Stampy actually stores how many stamps each user has awarded to every other user, and uses that to build a system of linear scalar equations which is then solved with numpy.\n* Each user has stamp points, and also gives a score to every other user they give stamps to the scores sum to 1 so if I give user A a stamp, my score for them will be 1.0, if I then give user B a stamp, my score for A is 0.5 and B is 0.5, if I give another to B, my score for A goes to 0.3333 and B to 0.66666 and so on\n* Score is \"what proportion of the stamps I've given have gone to this user\"\n* Everyone's stamp points is the sum of (every other user's score for them, times that user's stamp points) so the way to get points is to get stamps from people who have points\n* Rob is the root of the tree, he got one point from Stampy\n* So the idea is the stamp power kind of flows through the network, giving people points for posting things that I thought were good, or posting things that \"people who posted things I thought were good\" thought were good, and so on ad infinitum so for posting YouTube comments, Stampy won't send the comment until it has enough stamps of approval. Which could be a small number of high-points users or a larger number of lower-points users\n* Stamps given to yourself or to stampy do nothing\n\nSo yeah everyone ends up with a number that basically represents what Stampy thinks of them, and you can ask him \"how many stamps am I worth?\" to get that number\n\nso if you have people a, b, and c, the points are calculated by:<br/>\na_points \ua78a (bs_score_for_a * b_points) + (cs_score_for_a * c_points)<br/>\nb_points \ua78a (as_score_for_b * a_points) + (cs_score_for_b * c_points)<br/>\nc_points \ua78a (as_score_for_c * a_points) + (bs_score_for_c * b_points)<br/>\nwhich is tough because you need to know everyone else's score before you can calculate your own<br/>\nbut actually the system will have a fixed point - there'll be a certain arrangement of values such that every node has as much flowing out as flowing in - a stable configuration\nso you can rearrange<br/>\n(bs_score_for_a * b_points) + (cs_score_for_a * c_points) - a_points \ua78a 0<br/>\n(as_score_for_b * a_points) + (cs_score_for_b * c_points) - b_points \ua78a 0<br/>\n(as_score_for_c * a_points) + (bs_score_for_c * b_points) - c_points \ua78a 0<br/>\nor, for neatness:<br/>\n(     -1        * a_points) + (bs_score_for_a * b_points) + (cs_score_for_a * c_points) \ua78a 0<br/>\n(as_score_for_b * a_points) + (     -1        * b_points) + (cs_score_for_b * c_points) \ua78a 0<br/>\n(as_score_for_c * a_points) + (bs_score_for_c * b_points) + (     -1        * c_points) \ua78a 0 <br/>\nand this is just a system of linear scalar equations that you can throw at numpy.linalg.solve<br/>\n(you add one more equation that says rob_points \ua78a 1, so there's some place to start from)\nthere should be one possible distribution of points such that all of the equations hold at the same time, and numpy finds that by linear algebra magic beyond my very limited understanding<br/>\nbut as far as I can tell you can have all the cycles you want!<br/>\n(I actually have the scores sum to slightly less than 1, to have the stamp power slightly fade out as it propagates, just to make sure it doesn't explode. But I don't think I actually need to do that)<br/>\nand yes this means that any time anyone gives a stamp to anyone, ~everyone's points will change slightly<br/>\nAnd yes this means I'm recalculating the matrix and re-solving it for every new stamp, but computers are fast and I'm sure there are cheaper approximations I could switch to later if necessary"],"AnswerTo":[{"fulltext":"How does the stamp eigenkarma system work?","fullurl":"https://stampy.ai/wiki/How_does_the_stamp_eigenkarma_system_work%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to How does the stamp eigenkarma system work?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_does_the_stamp_eigenkarma_system_work%3F","namespace":0,"exists":"1","displaytitle":"Answer to How does the stamp eigenkarma system work?"},"Plex's Answer to How doomed is humanity?":{"printouts":{"Answer":["The opinions from experts are all over the place, according to \uff3bhttps://www.lesswrong.com/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results this 2021 survey\uff3d. Someone has also collected a \uff3bhttps://docs.google.com/spreadsheets/d/1W10B6NJjicD8O0STPiT3tNV3oFnT8YsfjmtYR8RO_RI/edit#gid\ua78a0 database of existential risk estimates\uff3d.\n\nOn the pessimistic end you find people like Eliezer Yudkowsky, \uff3bhttps://forum.effectivealtruism.org/posts/bGBm2yTiLEwwCbL6w/discussion-with-eliezer-yudkowsky-on-agi-interventions who said\uff3d: \"I consider the present gameboard to look incredibly grim, and I don't actually see a way out through hard work alone. We can hope there's a miracle that violates some aspect of my background model, and we can try to prepare for that unknown miracle; preparing for an unknown miracle probably looks like \"Trying to die with more dignity on the mainline\" (because if you can die with more dignity on the mainline, you are better positioned to take advantage of a miracle if it occurs).\"\n\nWhile at the optimistic end you have people like Ben Garfinkel who put the probability at more like 0.1-1% for AI causing an existential catastrophe in the next century, with most people lying somewhere in the middle."],"AnswerTo":[{"fulltext":"How doomed is humanity?","fullurl":"https://stampy.ai/wiki/How_doomed_is_humanity%3F","namespace":0,"exists":"1","displaytitle":"How doomed is humanity?"}]},"fulltext":"Plex's Answer to How doomed is humanity?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_doomed_is_humanity%3F","namespace":0,"exists":"1","displaytitle":"Answer to How doomed is humanity?"},"Helenator's Answer to How fast will AI takeoff be?":{"printouts":{"Answer":["There is significant controversy on how quickly AI will grow into a superintelligence. The \uff3bhttps://www.alignmentforum.org/tag/ai-takeoff Alignment Forum tag\uff3d has many views on how things might unfold, where the probabilities of a soft (happening over years/decades) takeoff and a hard (happening in months, or less) takeoff are discussed."],"AnswerTo":[{"fulltext":"How fast will AI takeoff be?","fullurl":"https://stampy.ai/wiki/How_fast_will_AI_takeoff_be%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Helenator's Answer to How fast will AI takeoff be?","fullurl":"https://stampy.ai/wiki/Helenator%27s_Answer_to_How_fast_will_AI_takeoff_be%3F","namespace":0,"exists":"1","displaytitle":"Answer to How fast will AI takeoff be?"},"Plex's Answer to How is AGI different from current AI?":{"printouts":{"Answer":["Current narrow systems are much more domain-specific than AGI. We don\u2019t know what the first AGI will look like, some people think the \uff3bhttps://www.lesswrong.com/posts/6Hee7w2paEzHsD6mn/collection-of-gpt-3-results GPT-3\uff3d architecture but scaled up a lot may get us there (GPT-3 is a giant prediction model which when trained on a vast amount of text seems to \uff3bhttps://www.lesswrong.com/posts/D3hP47pZwXNPRByj8/an-102-meta-learning-by-gpt-3-and-a-list-of-full-proposals learn how to learn\uff3d and do \uff3bhttps://gpt3examples.com/ all sorts of crazy-impressive things\uff3d, a related model can \uff3bhttps://openai.com/blog/dall-e/ generate pictures from text\uff3d), some people don\u2019t think scaling this kind of model will get us all the way."],"AnswerTo":[{"fulltext":"How is AGI different from current AI?","fullurl":"https://stampy.ai/wiki/How_is_AGI_different_from_current_AI%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to How is AGI different from current AI?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_is_AGI_different_from_current_AI%3F","namespace":0,"exists":"1","displaytitle":"Answer to How is AGI different from current AI?"},"Answer to How is \u2018intelligence\u2019 defined?":{"printouts":{"Answer":["<p>After reviewing extensive literature on the subject, Legg and Hutter<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnrefosnb04qur8\"><sup>(ref)<p>http://arxiv.org/pdf/0712.3329.pdf</p>(/ref)</sup></span>&nbsp;summarizes the many possible valuable definitions in the informal statement \u201cIntelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.\u201d They then show this definition can be mathematically formalized given reasonable mathematical definitions of its terms. They use \uff3bhttps://lessestwrong.com/tag/solomonoff-induction Solomonoff induction\uff3d - a formalization of \uff3bhttps://lessestwrong.com/tag/occam-s-razor Occam's razor\uff3d - to construct an \uff3bhttps://lessestwrong.com/tag/aixi universal artificial intelligence\uff3d with a embedded \uff3bhttps://lessestwrong.com/tag/utility-functions utility function\uff3d which assigns less \uff3bhttps://lessestwrong.com/tag/expected-utility utility\uff3d to those actions based on theories with higher \uff3bhttps://wiki.lesswrong.com/wiki/Kolmogorov_complexity complexity\uff3d. They argue this final formalization is a valid, meaningful, informative, general, unbiased, fundamental, objective, universal and practical definition of intelligence.</p><p>We can relate Legg and Hutter's definition with the concept of \uff3bhttps://lessestwrong.com/tag/optimization optimization\uff3d. According to \uff3bhttps://lessestwrong.com/tag/eliezer-yudkowsky Eliezer Yudkowsky\uff3d intelligence is \uff3bhttps://lessestwrong.com/lw/vb/efficient_crossdomain_optimization/ efficient cross-domain optimization\uff3d. It measures an agent's capacity for efficient cross-domain optimization of the world according to the agent\u2019s preferences.<span class\ua78a\"footnote-reference\" role\ua78a\"doc-noteref\" id\ua78a\"fnref7hbpdfpe6x3\"><sup>(ref)<p>http://intelligence.org/files/IE-EI.pdf(/ref)</sup></span>&nbsp;Optimization measures not only the capacity to achieve the desired goal but also is inversely proportional to the amount of resources used. It\u2019s the ability to steer the future so it hits that small target of desired outcomes in the large space of all possible outcomes, using fewer resources as possible. For example, when Deep Blue defeated Kasparov, it was able to hit that small possible outcome where it made the right order of moves given Kasparov\u2019s moves from the very large set of all possible moves. In that domain, it was more optimal than Kasparov. However, Kasparov would have defeated Deep Blue in almost any other relevant domain, and hence, he is considered more intelligent.</p><p>One could cast this definition in a possible world vocabulary, intelligence is:</p><ol><li>the ability to precisely realize one of the members of a small set of possible future worlds that have a higher preference over the vast set of all other possible worlds with lower preference; while</li><li>using fewer resources than the other alternatives paths for getting there; and in the</li><li>most diverse domains as possible.</li></ol><p>How many more worlds have a higher preference then the one realized by the agent, less intelligent he is. How many more worlds have a lower preference than the one realized by the agent, more intelligent he is. (Or: How much smaller is the set of worlds at least as preferable as the one realized, more intelligent the agent is). How much less paths for realizing the desired world using fewer resources than those spent by the agent, more intelligent he is. And finally, in how many more domains the agent can be more efficiently optimal, more intelligent he is. Restating it, the intelligence of an agent is directly proportional to:</p><ul><li>(a) the numbers of worlds with lower preference than the one realized,</li><li>(b) how much smaller is the set of paths more efficient than the one taken by the agent and</li><li>(c) how more wider are the domains where the agent can effectively realize his preferences;</li></ul><p>and it is, accordingly, inversely proportional to:</p><ul><li>(d) the numbers of world with higher preference than the one realized,</li><li>(e) how much bigger is the set of paths more efficient than the one taken by the agent and</li><li>(f) how much more narrow are the domains where the agent can efficiently realize his preferences.</li></ul><p>This definition avoids several problems common in many others definitions, especially it avoids \uff3bhttps://lessestwrong.com/tag/anthropomorphism anthropomorphizing\uff3d intelligence.</p><h2>See Also</h2><ul><li>\uff3bhttps://lessestwrong.com/tag/optimization Optimization process\uff3d</li><li>\uff3bhttps://lessestwrong.com/tag/decision-theory Decision theory\uff3d</li><li>\uff3bhttps://lessestwrong.com/tag/rationality Rationality\uff3d</li><li>\uff3bhttp://arxiv.org/pdf/0712.3329.pdf Legg and Hutter paper \u201cUniversal Intelligence: A De\ufb01nition of Machine Intelligence\u201d\uff3d</li></ul><div class\ua78a\"mw-editsection\" style\ua78a\"display: inline-block;\">\uff3bhttps://www.lesswrong.com/tag/general-intelligence?edit\ua78atrue Edit\uff3d</div>"],"AnswerTo":[{"fulltext":"How is \u2018intelligence\u2019 defined?","fullurl":"https://stampy.ai/wiki/How_is_%E2%80%98intelligence%E2%80%99_defined%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Answer to How is \u2018intelligence\u2019 defined?","fullurl":"https://stampy.ai/wiki/Answer_to_How_is_%E2%80%98intelligence%E2%80%99_defined%3F","namespace":0,"exists":"1","displaytitle":"Answer to How is \u2018intelligence\u2019 defined?"},"Plex's Answer to How likely is an intelligence explosion?":{"printouts":{"Answer":["Conditional on technological progress continuing, it seems extremely likely that there will be an intelligence explosion, as at some point generally capable intelligent systems will tend to become the main drivers of their own development both at a software and hardware level. This would predictably create a feedback cycle of more and more intelligent systems improving themselves more effectively.  It seems like if the compute was used effectively, \uff3bhttps://publicism.info/philosophy/superintelligence/4.html computers have many large advantages over biological cognition\uff3d, so this scaling up might be very rapid.\n\nSome ways technological progress could stop would be global coordination to stop AI research, disasters which permanently stop us from developing, or hardware reaching physical limits before an intelligence explosion is possible (though this last one seems unlikely, as \uff3bhttps://en.wikipedia.org/wiki/Atomically_precise_manufacturing atomically precise manufacturing\uff3d promises many orders of magnitude of cost reduction and processing power increase)."],"AnswerTo":[{"fulltext":"How likely is an intelligence explosion?","fullurl":"https://stampy.ai/wiki/How_likely_is_an_intelligence_explosion%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to How likely is an intelligence explosion?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_likely_is_an_intelligence_explosion%3F","namespace":0,"exists":"1","displaytitle":"Answer to How likely is an intelligence explosion?"},"Plex's Answer to How likely is it that an AI would pretend to be a human to further its goals?":{"printouts":{"Answer":["Talking about full AGI: Fairly likely, but depends on takeoff speed. In a slow takeoff of a misaligned AGI, where it is only weakly superintelligent, manipulating humans would be one of its main options for trying to further its goals for some time. Even in a fast takeoff, it\u2019s plausible that it would at least briefly manipulate humans in order to accelerate its ascent to technological superiority, though depending on what machines are available to hack at the time it may be able to skip this stage.\n\nIf the AI's goals include reference to humans it may have reason to continue deceiving us after it attains technological superiority, but will not necessarily do so. How this unfolds would depend on the details of its goals.\n\nEliezer Yudkowsky gives \uff3bhttps://www.lesswrong.com/posts/pxGYZs2zHJNHvWY5b/request-for-concrete-ai-takeover-mechanisms the example\uff3d of an AI solving protein folding, then mail-ordering synthesised DNA to a bribed or deceived human with instructions to mix the ingredients in a specific order to create \uff3bhttps://en.wikipedia.org/wiki/Atomically_precise_manufacturing wet nanotechnology\uff3d."],"AnswerTo":[{"fulltext":"How likely is it that an AI would pretend to be a human to further its goals?","fullurl":"https://stampy.ai/wiki/How_likely_is_it_that_an_AI_would_pretend_to_be_a_human_to_further_its_goals%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to How likely is it that an AI would pretend to be a human to further its goals?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_likely_is_it_that_an_AI_would_pretend_to_be_a_human_to_further_its_goals%3F","namespace":0,"exists":"1","displaytitle":"Answer to How likely is it that an AI would pretend to be a human to further its goals?"},"Plex's Answer to How might AGI kill people?":{"printouts":{"Answer":["If we pose a serious threat, it could hack our weapons systems and turn them against us. Future militaries are much more vulnerable to this due to rapidly progressing autonomous weapons. There\u2019s also the option of creating bioweapons and distributing them to the most unstable groups you can find, tricking nations into WW3, or dozens of other things an agent many times smarter than any human with the ability to develop arbitrary technology, hack things (including communications), and manipulate people, or many other possibilities that something smarter than a human could think up. More can be found \uff3bhttps://www.lesswrong.com/posts/pxGYZs2zHJNHvWY5b/request-for-concrete-ai-takeover-mechanisms here\uff3d.\n \nIf we are not a threat, in the course of pursuing its goals it may consume vital resources that humans need (e.g. using land for solar panels instead of farm crops). See \uff3bhttps://www.youtube.com/watch?v\ua78aZeecOKBus3Q this video\uff3d for more details."],"AnswerTo":[{"fulltext":"How might AGI kill people?","fullurl":"https://stampy.ai/wiki/How_might_AGI_kill_people%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to How might AGI kill people?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_might_AGI_kill_people%3F","namespace":0,"exists":"1","displaytitle":"Answer to How might AGI kill people?"},"Answer to How might a superintelligence socially manipulate humans?":{"printouts":{"Answer":["People tend to imagine AIs as being like nerdy humans \u2013 brilliant at technology but clueless about social skills. There is no reason to expect this \u2013 persuasion and manipulation is a different kind of skill from solving mathematical proofs, but it\u2019s still a skill, and an intellect as far beyond us as we are beyond lions might be smart enough to replicate or exceed the \u201ccharming sociopaths\u201d who can naturally win friends and followers despite a lack of normal human emotions.\n\nA superintelligence might be able to analyze human psychology deeply enough to understand the hopes and fears of everyone it negotiates with. Single humans using psychopathic social manipulation have done plenty of harm \u2013 Hitler leveraged his skill at oratory and his understanding of people\u2019s darkest prejudices to take over a continent. Why should we expect superintelligences to do worse than humans far less skilled than they?\n\nMore outlandishly, a superintelligence might just skip language entirely and figure out a weird pattern of buzzes and hums that causes conscious thought to seize up, and which knocks anyone who hears it into a weird hypnotizable state in which they\u2019ll do anything the superintelligence asks. It sounds kind of silly to me, but then, nuclear weapons probably would have sounded kind of silly to lions sitting around speculating about what humans might be able to accomplish. When you\u2019re dealing with something unbelievably more intelligent than you are, you should probably expect the unexpected."],"AnswerTo":[{"fulltext":"How might a superintelligence socially manipulate humans?","fullurl":"https://stampy.ai/wiki/How_might_a_superintelligence_socially_manipulate_humans%3F","namespace":0,"exists":"1","displaytitle":"How might a superintelligence socially manipulate humans?"}]},"fulltext":"Answer to How might a superintelligence socially manipulate humans?","fullurl":"https://stampy.ai/wiki/Answer_to_How_might_a_superintelligence_socially_manipulate_humans%3F","namespace":0,"exists":"1","displaytitle":"Answer to How might a superintelligence socially manipulate humans?"},"Answer to How might an AI achieve a seemingly beneficial goal via inappropriate means?":{"printouts":{"Answer":["Imagine, for example, that you are tasked with reducing traffic congestion in San Francisco at all costs, i.e. you do not take into account any other constraints. How would you do it? You might start by just timing traffic lights better. But wouldn\u2019t there be less traffic if all the bridges closed down from 5 to 10AM, preventing all those cars from entering the city? Such a measure obviously violates common sense, and subverts the purpose of improving traffic, which is to help people get around \u2013 but it is consistent with the goal of \u201creducing traffic congestion\u201d."],"AnswerTo":[{"fulltext":"How might an AI achieve a seemingly beneficial goal via inappropriate means?","fullurl":"https://stampy.ai/wiki/How_might_an_AI_achieve_a_seemingly_beneficial_goal_via_inappropriate_means%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Answer to How might an AI achieve a seemingly beneficial goal via inappropriate means?","fullurl":"https://stampy.ai/wiki/Answer_to_How_might_an_AI_achieve_a_seemingly_beneficial_goal_via_inappropriate_means%3F","namespace":0,"exists":"1","displaytitle":"Answer to How might an AI achieve a seemingly beneficial goal via inappropriate means?"},"Answer to How might an intelligence explosion be dangerous?":{"printouts":{"Answer":["If programmed with the wrong motivations, a machine could be malevolent toward humans, and intentionally exterminate our species. More likely, it could be designed with motivations that initially appeared safe (and easy to program) to its designers, but that turn out to be best fulfilled (given sufficient power) by reallocating resources from sustaining human life to \uff3bhttp://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf other projects\uff3d. As Yudkowsky writes, \u201cthe AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\u201d\n\nSince weak AIs with many different motivations could better achieve their goal by faking benevolence until they are powerful, safety testing to avoid this could be very challenging. Alternatively, competitive pressures, both economic and military, might lead AI designers to try to use other methods to control AIs with undesirable motivations. As those AIs became more sophisticated this could eventually lead to one risk too many.\n\nEven a machine successfully designed with superficially benevolent motivations could easily go awry when it discovers implications of its decision criteria unanticipated by its designers. For example, a superintelligence programmed to maximize human happiness might find it easier to rewire human neurology so that humans are happiest when sitting quietly in jars than to build and maintain a utopian world that caters to the complex and nuanced whims of current human neurology.\n\nSee also:\n\n* Yudkowsky, \uff3bhttps://intelligence.org/files/AIPosNegFactor.pdf Artificial intelligence as a positive and negative factor in global risk\uff3d\n* Chalmers, \uff3bhttp://consc.net/papers/singularity.pdf The Singularity: A Philosophical Analysis\uff3d"],"AnswerTo":[{"fulltext":"How might an intelligence explosion be dangerous?","fullurl":"https://stampy.ai/wiki/How_might_an_intelligence_explosion_be_dangerous%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Answer to How might an intelligence explosion be dangerous?","fullurl":"https://stampy.ai/wiki/Answer_to_How_might_an_intelligence_explosion_be_dangerous%3F","namespace":0,"exists":"1","displaytitle":"Answer to How might an intelligence explosion be dangerous?"},"Plex's Answer to How might non-agentic GPT-style AI cause an intelligence explosion or otherwise contribute to existential risk?":{"printouts":{"Answer":["One threat model which includes a GPT component is \uff3bhttps://www.lesswrong.com/posts/zzXawbXDwCZobwF9D/my-agi-threat-model-misaligned-model-based-rl-agent Misaligned Model-Based RL Agent\uff3d. It suggests that a reinforcement learner attached to a GPT-style world model could lead to an existential risk, with the RL agent being the optimizer which uses the world model to be much more effective at achieving its goals.\n\nAnother possibility is that a sufficiently powerful world model \uff3bhttps://www.lesswrong.com/posts/BGD5J2KAoNmpPMzMQ/why-gpt-wants-to-mesa-optimize-and-how-we-might-change-this may develop mesa optimizers\uff3d which could influence the world via the outputs of the model to achieve the mesa objective (perhaps by causing an optimizer to be created with goals aligned to it), though this is somewhat speculative."],"AnswerTo":[{"fulltext":"How might non-agentic GPT-style AI cause an intelligence explosion or otherwise contribute to existential risk?","fullurl":"https://stampy.ai/wiki/How_might_non-agentic_GPT-style_AI_cause_an_intelligence_explosion_or_otherwise_contribute_to_existential_risk%3F","namespace":0,"exists":"1","displaytitle":"How might non-agentic GPT-style AI cause an intelligence explosion or otherwise contribute to existential risk?"}]},"fulltext":"Plex's Answer to How might non-agentic GPT-style AI cause an intelligence explosion or otherwise contribute to existential risk?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_might_non-agentic_GPT-style_AI_cause_an_intelligence_explosion_or_otherwise_contribute_to_existential_risk%3F","namespace":0,"exists":"1","displaytitle":"Answer to How might non-agentic GPT-style AI cause an intelligence explosion or otherwise contribute to existential risk?"},"Plex's Answer to How quickly could an AI go from the first indications of problems to an unrecoverable disaster?":{"printouts":{"Answer":["If the AI system was deceptively aligned (i.e. pretending to be nice until it was in control of the situation) or had been in stealth mode while getting things in place for a takeover, quite possibly within hours. We may get more warning with weaker systems, if the AGI does not feel at all threatened by us, or if a \uff3bhttps://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story complex ecosystem of AI systems is built over time and we gradually lose control\uff3d. \n\nPaul Christiano writes \uff3bhttps://www.lesswrong.com/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story a story of alignment failure\uff3d which shows a relatively fast transition."],"AnswerTo":[{"fulltext":"How quickly could an AI go from the first indications of problems to an unrecoverable disaster?","fullurl":"https://stampy.ai/wiki/How_quickly_could_an_AI_go_from_the_first_indications_of_problems_to_an_unrecoverable_disaster%3F","namespace":0,"exists":"1","displaytitle":"How quickly could an AI go from the first indications of problems to an unrecoverable disaster?"}]},"fulltext":"Plex's Answer to How quickly could an AI go from the first indications of problems to an unrecoverable disaster?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_How_quickly_could_an_AI_go_from_the_first_indications_of_problems_to_an_unrecoverable_disaster%3F","namespace":0,"exists":"1","displaytitle":"Answer to How quickly could an AI go from the first indications of problems to an unrecoverable disaster?"},"Mic's Answer to I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?":{"printouts":{"Answer":["OK, it\u2019s great that you want to help, here are some ideas for ways you could do so without making a huge commitment:\n\n* Learning more about AI alignment will provide you with good foundations for any path towards helping. You could start by absorbing content (e.g. books, videos, posts), and thinking about challenges or possible solutions.\n* Getting involved with the movement by joining a local Effective Altruism or LessWrong group, Rob Miles\u2019s Discord, and/or the AI Safety Slack is a great way to find friends who are interested and will help you stay motivated.\n* Donating to organizations or individuals working on AI alignment, possibly via a \uff3bhttps://funds.effectivealtruism.org/donor-lottery donor lottery\uff3d or the \uff3bhttps://funds.effectivealtruism.org/funds/far-future Long Term Future Fund\uff3d, can be a great way to provide support.\n* \uff3bhttps://stampy.ai/wiki/Answer_questions Writing\uff3d or \uff3bhttps://stampy.ai/wiki/Improve_answers improving answers\uff3d on \uff3bhttps://stampy.ai/wiki/ my wiki\uff3d so that other people can learn about AI alignment more easily is a great way to dip your toe into contributing. You can always ask on the Discord for feedback on things you write.\n* Getting good at giving an AI alignment elevator pitch, and sharing it with people who may be valuable to have working on the problem can make a big difference. However you should avoid putting them off the topic by presenting it in a way which causes them to dismiss it as sci-fi (dos and don\u2019ts in the elevator pitch follow-up question).\n* Writing thoughtful comments on \uff3bhttps://www.lesswrong.com/tag/ai?sortedBy\ua78amagic AI posts on LessWrong\uff3d.\n* Participating in the \uff3bhttps://www.eacambridge.org/agi-safety-fundamentals AGI Safety Fundamentals program\uff3d \u2013 either the AI alignment or governance track \u2013 and then facilitating discussions for it in the following round. The program involves nine weeks of content, with about two hours of readings + exercises per week and 1.5 hours of discussion, followed by four weeks to work on an independent project. As a facilitator, you'll be helping others learn about AI safety in-depth, many of whom are considering a career in AI safety. In the early 2022 round, facilitators were offered a stipend, and this seems likely to be the case for future rounds as well! You can learn more about facilitating in \uff3bhttps://forum.effectivealtruism.org/posts/WtwMy69JKZeHEvykc/contribute-by-facilitating-the-agi-safety-fundamentals this post from December 2021\uff3d."],"AnswerTo":[{"fulltext":"I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?","fullurl":"https://stampy.ai/wiki/I_want_to_help_out_AI_alignment_without_necessarily_making_major_life_changes._What_are_some_simple_things_I_can_do_to_contribute%3F","namespace":0,"exists":"1","displaytitle":"I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?"}]},"fulltext":"Mic's Answer to I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?","fullurl":"https://stampy.ai/wiki/Mic%27s_Answer_to_I_want_to_help_out_AI_alignment_without_necessarily_making_major_life_changes._What_are_some_simple_things_I_can_do_to_contribute%3F","namespace":0,"exists":"1","displaytitle":"Answer to I want to help out AI alignment without necessarily making major life changes. What are some simple things I can do to contribute?"},"Plex's Answer to I want to work on AI alignment. How can I get funding?":{"printouts":{"Answer":["See the \uff3bhttps://www.futurefundinglist.com/ Future Funding List\uff3d for up to date information!\n\nThe organizations which most regularly give grants to individuals working towards AI alignment are the \uff3bhttps://funds.effectivealtruism.org/funds/far-future Long Term Future Fund\uff3d, \uff3bhttp://survivalandflourishing.org/ Survival And Flourishing (SAF)\uff3d, the \uff3bhttps://www.openphilanthropy.org/focus/global-catastrophic-risks/potential-risks-advanced-artificial-intelligence/the-open-phil-ai-fellowship OpenPhil AI Fellowship\uff3d and \uff3bhttps://www.openphilanthropy.org/focus/other-areas/early-career-funding-individuals-interested-improving-long-term-future early career funding\uff3d, the \uff3bhttps://grants.futureoflife.org/ Future of Life Institute\uff3d, the \uff3bhttps://www.fhi.ox.ac.uk/aia-fellowship/ Future of Humanity Institute\uff3d, and \uff3bhttps://longtermrisk.org/grantmaking/ the Center on Long-Term Risk Fund\uff3d. If you're able to relocate to the UK, \uff3bhttps://ceealar.org/ CEEALAR (aka the EA Hotel)\uff3d can be a great option as it offers free food and accommodation for up to two years, as well as contact with others who are thinking about these issues. The \uff3bhttps://ftxfuturefund.org/apply/ FTX Future Fund\uff3d only accepts direct applications for $100k+ with an emphasis on massively scaleable interventions, but their \uff3bhttps://ftxfuturefund.org/announcing-our-regranting-program/ regranters\uff3d can make smaller grants for individuals. There are also opportunities from smaller grantmakers which you might be able to pick up if you get involved.\n\nIf you want to work on support or infrastructure rather than directly on research, the \uff3bhttps://funds.effectivealtruism.org/funds/ea-community EA Infrastructure Fund\uff3d may be able to help. In general, you can \uff3bhttps://www.lesswrong.com/posts/5AAFoigbbMqgrTpDh/you-can-talk-to-ea-funds-before-applying talk to EA funds before applying\uff3d.\n\nEach grant source has their own criteria for funding, but in general they are looking for candidates who have evidence that they're keen and able to do good work towards reducing existential risk (for example, by completing an \uff3bhttps://aisafety.camp/ AI Safety Camp\uff3d project), though the EA Hotel in particular has less stringent requirements as they're able to support people at very low cost. If you'd like to talk to someone who can offer advice on applying for funding, \uff3bhttps://www.aisafetysupport.org/ AI Safety Support\uff3d offers \uff3bhttps://calendly.com/aiss free calls\uff3d.\n\nAnother option is to get hired by an organization which works on AI alignment, see the follow-up question for advice on that.\n\nIt's also worth checking the AI Alignment tag on the \uff3bhttps://eafunding.softr.app/ EA funding sources website\uff3d for up-to-date suggestions."],"AnswerTo":[{"fulltext":"I want to work on AI alignment. How can I get funding?","fullurl":"https://stampy.ai/wiki/I_want_to_work_on_AI_alignment._How_can_I_get_funding%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to I want to work on AI alignment. How can I get funding?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_I_want_to_work_on_AI_alignment._How_can_I_get_funding%3F","namespace":0,"exists":"1","displaytitle":"Answer to I want to work on AI alignment. How can I get funding?"},"Filip's Answer to I'm interested in working on AI Safety. What should I do?":{"printouts":{"Answer":["AI Safety Support \uff3bhttps://calendly.com/aiss offers free calls\uff3d to advise people interested in a career in AI Safety, so that's a great place to start. We're working on creating a bunch of detailed information for Stampy to use, but in the meantime check out these resources:\n\n* \uff3bhttps://www.eacambridge.org/agi-safety-fundamentals EA Cambridge AGI Safety Fundamentals curriculum\uff3d\n* \uff3bhttps://80000hours.org/articles/ai-safety-syllabus/ 80,000 Hours AI safety syllabus\uff3d\n* \uff3bhttps://docs.google.com/document/d/1RFo7_9JVmt0z8RPwUjB-mUMgCMoUQmsaj2CM5aHvxCw/edit Adam Gleave's Careers in Beneficial AI Research document\uff3d\n* \uff3bhttps://rohinshah.com/faq-career-advice-for-ai-alignment-researchers/ Rohin Shah's FAQ on career advice for AI alignment researchers\uff3d\n* \uff3bhttps://www.aisafetysupport.org/ AI Safety Support\uff3d has lots of other good resources, such as their \uff3bhttps://www.aisafetysupport.org/resources/lots-of-links links page\uff3d, \uff3bhttps://www.google.com/url?q\ua78ahttps%3A%2F%2Fjoin.slack.com%2Ft%2Fai-alignment%2Fshared_invite%2Fzt-fkgwbd2b-kK50z~BbVclOZMM9UP44gw&sa\ua78aD&sntz\ua78a1&usg\ua78aAFQjCNEIiKykU7SJ9LhJBoE3FFaOFOhOSA slack\uff3d, \uff3bhttps://www.aisafetysupport.org/newsletter newsletter\uff3d, and  \uff3bhttps://www.aisafetysupport.org/events/online-events-calendar events calendar\uff3d.\n* \uff3bhttps://docs.google.com/spreadsheets/d/1JyxrfFFrzaQsS3AQ4qJ2aOLGj1aSkBaxkpZCqBX9BOY/edit#gid\ua78a0 Safety-aligned research training programs (under construction).\uff3d"],"AnswerTo":[{"fulltext":"I'm interested in working on AI Safety. What should I do?","fullurl":"https://stampy.ai/wiki/I%27m_interested_in_working_on_AI_Safety._What_should_I_do%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Filip's Answer to I'm interested in working on AI Safety. What should I do?","fullurl":"https://stampy.ai/wiki/Filip%27s_Answer_to_I%27m_interested_in_working_on_AI_Safety._What_should_I_do%3F","namespace":0,"exists":"1","displaytitle":"Answer to I'm interested in working on AI Safety. What should I do?"},"Plex's Answer to If AI takes over the world how could it create and maintain the infrastructure that humans currently provide?":{"printouts":{"Answer":["An unaligned AI would not eliminate humans until it had replacements for the manual labor they provide to maintain civilization (e.g. a more advanced version of \uff3bhttps://en.wikipedia.org/wiki/Tesla_Bot Tesla's Optimus\uff3d). Until that point, it might settle for technologically and socially manipulating humans."],"AnswerTo":[{"fulltext":"If AI takes over the world how could it create and maintain the infrastructure that humans currently provide?","fullurl":"https://stampy.ai/wiki/If_AI_takes_over_the_world_how_could_it_create_and_maintain_the_infrastructure_that_humans_currently_provide%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to If AI takes over the world how could it create and maintain the infrastructure that humans currently provide?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_If_AI_takes_over_the_world_how_could_it_create_and_maintain_the_infrastructure_that_humans_currently_provide%3F","namespace":0,"exists":"1","displaytitle":"Answer to If AI takes over the world how could it create and maintain the infrastructure that humans currently provide?"},"ElloMelon's Answer to If I only care about helping people alive today, does AI safety still matter?":{"printouts":{"Answer":["This largely depends on when you think AI will be advanced enough to constitute an immediate threat to humanity. This is difficult to estimate, but the field is surveyed at \uff3b\uff3bHow long will it be until transformative AI is created?\uff3d\uff3d, which comes to the conclusion that it is relatively widely believed that AI will transform the world in our lifetimes.\n\nWe probably shouldn't rely too strongly on these opinions as predicting the future is hard. But, due to the enormous damage a misaligned AGI could do, it's worth putting a great deal of effort towards AI alignment even if you just care about currently existing humans (such as yourself)."],"AnswerTo":[{"fulltext":"If I only care about helping people alive today, does AI safety still matter?","fullurl":"https://stampy.ai/wiki/If_I_only_care_about_helping_people_alive_today,_does_AI_safety_still_matter%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"ElloMelon's Answer to If I only care about helping people alive today, does AI safety still matter?","fullurl":"https://stampy.ai/wiki/ElloMelon%27s_Answer_to_If_I_only_care_about_helping_people_alive_today,_does_AI_safety_still_matter%3F","namespace":0,"exists":"1","displaytitle":"Answer to If I only care about helping people alive today, does AI safety still matter?"},"Plex's Answer to If we solve alignment, are we sure of a good future?":{"printouts":{"Answer":["If by \u201csolve alignment\u201d you mean build a sufficiently performance-competitive superintelligence which has the goal of \uff3bhttps://www.lesswrong.com/tag/coherent-extrapolated-volition Coherent Extrapolated Volition\uff3d or something else which captures human values, then yes. It would be able to deploy technology near the limits of physics (e.g. \uff3bhttps://en.wikipedia.org/wiki/Atomically_precise_manufacturing atomically precise manufacturing\uff3d) to solve most of the other problems which face us, and steer the future towards a highly positive path for \uff3bhttps://en.wikipedia.org/wiki/Timeline_of_the_far_future perhaps many billions of years\uff3d until the \uff3bhttps://en.wikipedia.org/wiki/Heat_death_of_the_universe heat death of the universe\uff3d (barring more esoteric x-risks like encounters with advanced hostile civilizations, \uff3bhttps://en.wikipedia.org/wiki/False_vacuum_decay false vacuum decay\uff3d, or \uff3bhttps://arxiv.org/ftp/arxiv/papers/1905/1905.05792.pdf simulation shutdown\uff3d).\n\nHowever, if you only have alignment of a superintelligence to a single human you still have the risk of misuse, so this should be at most a short-term solution. For example, what if Google creates a superintelligent AI, and it listens to the CEO of Google, and it\u2019s programmed to do everything exactly the way the CEO of Google would want? Even assuming that the CEO of Google has no hidden unconscious desires affecting the AI in unpredictable ways, this gives one person a lot of power."],"AnswerTo":[{"fulltext":"If we solve alignment, are we sure of a good future?","fullurl":"https://stampy.ai/wiki/If_we_solve_alignment,_are_we_sure_of_a_good_future%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to If we solve alignment, are we sure of a good future?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_If_we_solve_alignment,_are_we_sure_of_a_good_future%3F","namespace":0,"exists":"1","displaytitle":"Answer to If we solve alignment, are we sure of a good future?"},"Plex's Answer to Is AI alignment possible?":{"printouts":{"Answer":["Yes, if the superintelligence has goals which include humanity surviving then we would not be destroyed. If those goals are \uff3bhttps://www.lesswrong.com/tag/value-learning fully aligned\uff3d with human well-being, we would in fact find ourselves in a dramatically better place."],"AnswerTo":[{"fulltext":"Is AI alignment possible?","fullurl":"https://stampy.ai/wiki/Is_AI_alignment_possible%3F","namespace":0,"exists":"1","displaytitle":"Is AI alignment possible?"}]},"fulltext":"Plex's Answer to Is AI alignment possible?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_Is_AI_alignment_possible%3F","namespace":0,"exists":"1","displaytitle":"Answer to Is AI alignment possible?"},"Plex's Answer to Is donating small amounts to AI safety organisations going to make a non-negligible difference?":{"printouts":{"Answer":["Many parts of the AI alignment ecosystem are already well-funded, but a savvy donor can still make a difference by picking up grantmaking opportunities which are too small to catch the attention of the major funding bodies or are based on personal knowledge of the recipient.\n\nOne way to leverage a small amount of money to the potential of a large amount is to enter a \uff3bhttps://funds.effectivealtruism.org/donor-lottery donor lottery\uff3d, where you donate to win a chance to direct a much larger amount of money (with probability proportional to donation size). This means that the person directing the money will be allocating enough that it's worth their time to do more in-depth research.\n\nFor an overview of the work the major organizations are doing, see the \uff3bhttps://forum.effectivealtruism.org/posts/BNQMyWGCNWDdP2WyG/2021-ai-alignment-literature-review-and-charity-comparison 2021 AI Alignment Literature Review and Charity Comparison\uff3d. The \uff3bhttps://funds.effectivealtruism.org/funds/far-future Long-Term Future Fund\uff3d seems to be an outstanding place to donate based on that, as they are the organization which most other organizations are most excited to see funded."],"AnswerTo":[{"fulltext":"Is donating small amounts to AI safety organisations going to make a non-negligible difference?","fullurl":"https://stampy.ai/wiki/Is_donating_small_amounts_to_AI_safety_organisations_going_to_make_a_non-negligible_difference%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Plex's Answer to Is donating small amounts to AI safety organisations going to make a non-negligible difference?","fullurl":"https://stampy.ai/wiki/Plex%27s_Answer_to_Is_donating_small_amounts_to_AI_safety_organisations_going_to_make_a_non-negligible_difference%3F","namespace":0,"exists":"1","displaytitle":"Answer to Is donating small amounts to AI safety organisations going to make a non-negligible difference?"},"Answer to Is expecting large returns from AI self-improvement just following an exponential trend line off a cliff?":{"printouts":{"Answer":["Blindly following the trendlines while forecasting technological progress is certainly a risk (affectionately known in AI circles as \u201cpulling a Kurzweill\u201d), but sometimes taking an exponential trend seriously is the right response.\n\nConsider economic doubling times. In 1 AD, the world GDP was about $20 billion; it took a thousand years, until 1000 AD, for that to double to $40 billion. But it only took five hundred more years, until 1500, or so, for the economy to double again. And then it only took another three hundred years or so, until 1800, for the economy to double a third time. \nSomeone in 1800 might calculate the trend line and say this was ridiculous, that it implied the economy would be doubling every ten years or so in the beginning of the 21st century. But in fact, this is how long the economy takes to double these days. To a medieval, used to a thousand-year doubling time (which was based mostly on population growth!), an economy that doubled every ten years might seem inconceivable. To us, it seems normal.\n\nLikewise, in 1965 Gordon Moore noted that semiconductor complexity seemed to double every eighteen months. During his own day, there were about five hundred transistors on a chip; he predicted that would soon double to a thousand, and a few years later to two thousand. \nAlmost as soon as Moore\u2019s Law become well-known, people started saying it was absurd to follow it off a cliff \u2013 such a law would imply a million transistors per chip in 1990, a hundred million in 2000, ten billion transistors on every chip by 2015! More transistors on a single chip than existed on all the computers in the world! Transistors the size of molecules! But of course all of these things happened; the ridiculous exponential trend proved more accurate than the naysayers.\n\nNone of this is to say that exponential trends are always right, just that they are sometimes right even when it seems they can\u2019t possibly be. We can\u2019t be sure that a computer using its own intelligence to discover new ways to increase its intelligence will enter a positive feedback loop and achieve superintelligence in seemingly impossibly short time scales. It\u2019s just one more possibility, a worry to place alongside all the other worrying reasons to expect a moderate or hard takeoff."],"AnswerTo":[{"fulltext":"Is expecting large returns from AI self-improvement just following an exponential trend line off a cliff?","fullurl":"https://stampy.ai/wiki/Is_expecting_large_returns_from_AI_self-improvement_just_following_an_exponential_trend_line_off_a_cliff%3F","namespace":0,"exists":"1","displaytitle":""}]},"fulltext":"Answer to Is expecting large returns from AI self-improvement just following an exponential trend line off a cliff?","fullurl":"https://stampy.ai/wiki/Answer_to_Is_expecting_large_returns_from_AI_self-improvement_just_following_an_exponential_trend_line_off_a_cliff%3F","namespace":0,"exists":"1","displaytitle":"Answer to Is expecting large returns from AI self-improvement just following an exponential trend line off a cliff?"}},"serializer":"SMW\\Serializers\\QueryResultSerializer","version":2,"meta":{"hash":"23b991a7f389ff4ebc592d0b7e0b0fb6","count":50,"offset":0,"source":"","time":"0.029892"}}}