## Stampy Wiki Duplicate Questions
Language model name: multi-qa-mpnet-base-dot-v1

Date generated: 2022-06-05 22:40:29.681376

| Question1 | Question2 | Score |
| :--- | :--- | :--- |
| [Who helped create Stampy?](https://stampy.ai/wiki/Who_helped_create_Stampy%3F) | [Who created Stampy?](https://stampy.ai/wiki/Who_created_Stampy%3F) | 0.98 |
| [Is humanity doomed?](https://stampy.ai/wiki/Is_humanity_doomed%3F) | [How doomed is humanity?](https://stampy.ai/wiki/How_doomed_is_humanity%3F) | 0.95 |
| [What is a canonical question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_canonical_question_on_Stampy%27s_Wiki%3F) | [What is a canonical version of a question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_canonical_version_of_a_question_on_Stampy%27s_Wiki%3F) | 0.93 |
| [Why can’t we just “put the AI in a box” so it can’t influence the outside world?](https://stampy.ai/wiki/Why_can%E2%80%99t_we_just_%E2%80%9Cput_the_AI_in_a_box%E2%80%9D_so_it_can%E2%80%99t_influence_the_outside_world%3F) | [Couldn’t we keep the AI in a box and never give it the ability to manipulate the external world?](https://stampy.ai/wiki/Couldn%E2%80%99t_we_keep_the_AI_in_a_box_and_never_give_it_the_ability_to_manipulate_the_external_world%3F) | 0.92 |
| [How might a superintelligence technologically manipulate humans?](https://stampy.ai/wiki/How_might_a_superintelligence_technologically_manipulate_humans%3F) | [How might a superintelligence socially manipulate humans?](https://stampy.ai/wiki/How_might_a_superintelligence_socially_manipulate_humans%3F) | 0.92 |
| [Why is AI Safety important?](https://stampy.ai/wiki/Why_is_AI_Safety_important%3F) | [Why is safety important for smarter-than-human AI?](https://stampy.ai/wiki/Why_is_safety_important_for_smarter-than-human_AI%3F) | 0.91 |
| [Can we tell an AI just to figure out what we want, then do that?](https://stampy.ai/wiki/Can_we_tell_an_AI_just_to_figure_out_what_we_want,_then_do_that%3F) | [Can we just tell an AI to do what we want?](https://stampy.ai/wiki/Can_we_just_tell_an_AI_to_do_what_we_want%3F) | 0.90 |
| [What is AI Safety?](https://stampy.ai/wiki/What_is_AI_Safety%3F) | [Why is AI Safety important?](https://stampy.ai/wiki/Why_is_AI_Safety_important%3F) | 0.90 |
| [I’d like a good introduction to AI alignment. Where can I find one?](https://stampy.ai/wiki/I%E2%80%99d_like_a_good_introduction_to_AI_alignment._Where_can_I_find_one%3F) | [What are good resources on AI alignment?](https://stampy.ai/wiki/What_are_good_resources_on_AI_alignment%3F) | 0.89 |
| [I’d like to get deeper into the AI alignment literature. Where should I look?](https://stampy.ai/wiki/I%E2%80%99d_like_to_get_deeper_into_the_AI_alignment_literature._Where_should_I_look%3F) | [What are good resources on AI alignment?](https://stampy.ai/wiki/What_are_good_resources_on_AI_alignment%3F) | 0.89 |
| [I’d like to get deeper into the AI alignment literature. Where should I look?](https://stampy.ai/wiki/I%E2%80%99d_like_to_get_deeper_into_the_AI_alignment_literature._Where_should_I_look%3F) | [I’d like a good introduction to AI alignment. Where can I find one?](https://stampy.ai/wiki/I%E2%80%99d_like_a_good_introduction_to_AI_alignment._Where_can_I_find_one%3F) | 0.89 |
| [Who is Stampy?](https://stampy.ai/wiki/Who_is_Stampy%3F) | [Who created Stampy?](https://stampy.ai/wiki/Who_created_Stampy%3F) | 0.88 |
| [Will there be an AI assisted long reflection and how might it look like?](https://stampy.ai/wiki/Will_there_be_an_AI_assisted_long_reflection_and_how_might_it_look_like%3F) | [How might an AI enabled long reflection look?](https://stampy.ai/wiki/How_might_an_AI_enabled_long_reflection_look%3F) | 0.88 |
| [What is a canonical question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_canonical_question_on_Stampy%27s_Wiki%3F) | [What should be marked as a canonical answer on Stampy's Wiki?](https://stampy.ai/wiki/What_should_be_marked_as_a_canonical_answer_on_Stampy%27s_Wiki%3F) | 0.88 |
| [How does AI taking things literally contribute to alignment being hard?](https://stampy.ai/wiki/How_does_AI_taking_things_literally_contribute_to_alignment_being_hard%3F) | [Why is AI alignment hard?](https://stampy.ai/wiki/Why_is_AI_alignment_hard%3F) | 0.87 |
| [What is Evidential Decision Theory?](https://stampy.ai/wiki/What_is_Evidential_Decision_Theory%3F) | [What is Logical Decision Theory?](https://stampy.ai/wiki/What_is_Logical_Decision_Theory%3F) | 0.87 |
| [When will an intelligence explosion happen?](https://stampy.ai/wiki/When_will_an_intelligence_explosion_happen%3F) | [Might an intelligence explosion never occur?](https://stampy.ai/wiki/Might_an_intelligence_explosion_never_occur%3F) | 0.86 |
| [How likely is an intelligence explosion?](https://stampy.ai/wiki/How_likely_is_an_intelligence_explosion%3F) | [When will an intelligence explosion happen?](https://stampy.ai/wiki/When_will_an_intelligence_explosion_happen%3F) | 0.86 |
| [How likely is an intelligence explosion?](https://stampy.ai/wiki/How_likely_is_an_intelligence_explosion%3F) | [Might an intelligence explosion never occur?](https://stampy.ai/wiki/Might_an_intelligence_explosion_never_occur%3F) | 0.86 |
| [Who is Stampy?](https://stampy.ai/wiki/Who_is_Stampy%3F) | [Who helped create Stampy?](https://stampy.ai/wiki/Who_helped_create_Stampy%3F) | 0.86 |
| [Why does there seem to have been an explosion of activity in AI in recent years?](https://stampy.ai/wiki/Why_does_there_seem_to_have_been_an_explosion_of_activity_in_AI_in_recent_years%3F) | [Why is the future of AI suddenly in the news? What has changed?](https://stampy.ai/wiki/Why_is_the_future_of_AI_suddenly_in_the_news%3F_What_has_changed%3F) | 0.86 |
| [How could an intelligence explosion be useful?](https://stampy.ai/wiki/How_could_an_intelligence_explosion_be_useful%3F) | [How might an intelligence explosion be dangerous?](https://stampy.ai/wiki/How_might_an_intelligence_explosion_be_dangerous%3F) | 0.86 |
| [Where can I find questions to answer for Stampy?](https://stampy.ai/wiki/Where_can_I_find_questions_to_answer_for_Stampy%3F) | [What kind of questions do we want on Stampy?](https://stampy.ai/wiki/What_kind_of_questions_do_we_want_on_Stampy%3F) | 0.86 |
| [What's especially worrisome about autonomous weapons?](https://stampy.ai/wiki/What%27s_especially_worrisome_about_autonomous_weapons%3F) | [Isn't the real concern autonomous weapons?](https://stampy.ai/wiki/Isn%27t_the_real_concern_autonomous_weapons%3F) | 0.86 |
| [How likely is an intelligence explosion?](https://stampy.ai/wiki/How_likely_is_an_intelligence_explosion%3F) | [How might an intelligence explosion be dangerous?](https://stampy.ai/wiki/How_might_an_intelligence_explosion_be_dangerous%3F) | 0.85 |
| [What is a canonical question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_canonical_question_on_Stampy%27s_Wiki%3F) | [What is a duplicate question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_duplicate_question_on_Stampy%27s_Wiki%3F) | 0.85 |
| [What is AI alignment?](https://stampy.ai/wiki/What_is_AI_alignment%3F) | [What is the general nature of the concern about AI alignment?](https://stampy.ai/wiki/What_is_the_general_nature_of_the_concern_about_AI_alignment%3F) | 0.85 |
| [Can an AI really be smarter than humans?](https://stampy.ai/wiki/Can_an_AI_really_be_smarter_than_humans%3F) | [Why think that AI can outperform humans?](https://stampy.ai/wiki/Why_think_that_AI_can_outperform_humans%3F) | 0.85 |
| [I want to work on AI alignment. How can I get funding?](https://stampy.ai/wiki/I_want_to_work_on_AI_alignment._How_can_I_get_funding%3F) | [How can I get hired by an organization working on AI alignment?](https://stampy.ai/wiki/How_can_I_get_hired_by_an_organization_working_on_AI_alignment%3F) | 0.85 |
| [What is the intelligence explosion?](https://stampy.ai/wiki/What_is_the_intelligence_explosion%3F) | [How could an intelligence explosion be useful?](https://stampy.ai/wiki/How_could_an_intelligence_explosion_be_useful%3F) | 0.84 |
| [Why is AI alignment hard?](https://stampy.ai/wiki/Why_is_AI_alignment_hard%3F) | [What kind of a challenge is solving AI alignment?](https://stampy.ai/wiki/What_kind_of_a_challenge_is_solving_AI_alignment%3F) | 0.84 |
| [What is AI alignment?](https://stampy.ai/wiki/What_is_AI_alignment%3F) | [Is AI alignment possible?](https://stampy.ai/wiki/Is_AI_alignment_possible%3F) | 0.84 |
| [Wouldn't a superintelligence be smart enough not to make silly mistakes in its comprehension of our instructions?](https://stampy.ai/wiki/Wouldn%27t_a_superintelligence_be_smart_enough_not_to_make_silly_mistakes_in_its_comprehension_of_our_instructions%3F) | [Wouldn't a superintelligence be smart enough to know right from wrong?](https://stampy.ai/wiki/Wouldn%27t_a_superintelligence_be_smart_enough_to_know_right_from_wrong%3F) | 0.84 |
| [What is a canonical version of a question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_canonical_version_of_a_question_on_Stampy%27s_Wiki%3F) | [What is a duplicate question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_duplicate_question_on_Stampy%27s_Wiki%3F) | 0.84 |
| [What is the general nature of the concern about AI alignment?](https://stampy.ai/wiki/What_is_the_general_nature_of_the_concern_about_AI_alignment%3F) | [What are some of the most carefully thought out objections to AI alignment?](https://stampy.ai/wiki/What_are_some_of_the_most_carefully_thought_out_objections_to_AI_alignment%3F) | 0.84 |
| [What would an actually good solution to AI alignment look like?](https://stampy.ai/wiki/What_would_an_actually_good_solution_to_AI_alignment_look_like%3F) | [What’s a good AI alignment elevator pitch?](https://stampy.ai/wiki/What%E2%80%99s_a_good_AI_alignment_elevator_pitch%3F) | 0.84 |
| [What is Causal Decision Theory?](https://stampy.ai/wiki/What_is_Causal_Decision_Theory%3F) | [What is Logical Decision Theory?](https://stampy.ai/wiki/What_is_Logical_Decision_Theory%3F) | 0.83 |
| [How can I collect questions for Stampy?](https://stampy.ai/wiki/How_can_I_collect_questions_for_Stampy%3F) | [Where can I find questions to answer for Stampy?](https://stampy.ai/wiki/Where_can_I_find_questions_to_answer_for_Stampy%3F) | 0.83 |
| [Is AI alignment possible?](https://stampy.ai/wiki/Is_AI_alignment_possible%3F) | [Can we ever be sure an AI is aligned?](https://stampy.ai/wiki/Can_we_ever_be_sure_an_AI_is_aligned%3F) | 0.83 |
| [Are there AI alignment projects which governments could usefully put a very large amount of resources into?](https://stampy.ai/wiki/Are_there_AI_alignment_projects_which_governments_could_usefully_put_a_very_large_amount_of_resources_into%3F) | [Is there something useful we can ask governments to do for AI alignment?](https://stampy.ai/wiki/Is_there_something_useful_we_can_ask_governments_to_do_for_AI_alignment%3F) | 0.83 |
| [What are common objections to AI alignment and brief responses?](https://stampy.ai/wiki/What_are_common_objections_to_AI_alignment_and_brief_responses%3F) | [What are some of the most carefully thought out objections to AI alignment?](https://stampy.ai/wiki/What_are_some_of_the_most_carefully_thought_out_objections_to_AI_alignment%3F) | 0.83 |
| [What is the intelligence explosion?](https://stampy.ai/wiki/What_is_the_intelligence_explosion%3F) | [When will an intelligence explosion happen?](https://stampy.ai/wiki/When_will_an_intelligence_explosion_happen%3F) | 0.83 |
| [Is AI alignment possible?](https://stampy.ai/wiki/Is_AI_alignment_possible%3F) | [What would an actually good solution to AI alignment look like?](https://stampy.ai/wiki/What_would_an_actually_good_solution_to_AI_alignment_look_like%3F) | 0.83 |
| [I'm interested in working on AI Safety. What should I do?](https://stampy.ai/wiki/I%27m_interested_in_working_on_AI_Safety._What_should_I_do%3F) | [How do I know whether I'm a good fit for work on AI safety?](https://stampy.ai/wiki/How_do_I_know_whether_I%27m_a_good_fit_for_work_on_AI_safety%3F) | 0.83 |
| [Would AI alignment be hard with deep learning?](https://stampy.ai/wiki/Would_AI_alignment_be_hard_with_deep_learning%3F) | [Why is AI alignment hard?](https://stampy.ai/wiki/Why_is_AI_alignment_hard%3F) | 0.83 |
| [What are the different possible AI takeoff speeds?](https://stampy.ai/wiki/What_are_the_different_possible_AI_takeoff_speeds%3F) | [How fast will AI takeoff be?](https://stampy.ai/wiki/How_fast_will_AI_takeoff_be%3F) | 0.82 |
| [How can I contribute to Stampy?](https://stampy.ai/wiki/How_can_I_contribute_to_Stampy%3F) | [Why might contributing to Stampy be worth my time?](https://stampy.ai/wiki/Why_might_contributing_to_Stampy_be_worth_my_time%3F) | 0.82 |
| [What approaches are AI alignment organizations working on?](https://stampy.ai/wiki/What_approaches_are_AI_alignment_organizations_working_on%3F) | [What would an actually good solution to AI alignment look like?](https://stampy.ai/wiki/What_would_an_actually_good_solution_to_AI_alignment_look_like%3F) | 0.82 |
| [Why is AGI dangerous?](https://stampy.ai/wiki/Why_is_AGI_dangerous%3F) | [Why don't we just not build AGI if it's so dangerous?](https://stampy.ai/wiki/Why_don%27t_we_just_not_build_AGI_if_it%27s_so_dangerous%3F) | 0.82 |
| [What is a canonical version of a question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_canonical_version_of_a_question_on_Stampy%27s_Wiki%3F) | [What should be marked as a canonical answer on Stampy's Wiki?](https://stampy.ai/wiki/What_should_be_marked_as_a_canonical_answer_on_Stampy%27s_Wiki%3F) | 0.82 |
| [What is AI alignment?](https://stampy.ai/wiki/What_is_AI_alignment%3F) | [Why is AI alignment hard?](https://stampy.ai/wiki/Why_is_AI_alignment_hard%3F) | 0.82 |
| [What is Logical Decision Theory?](https://stampy.ai/wiki/What_is_Logical_Decision_Theory%3F) | [What is Functional Decision Theory?](https://stampy.ai/wiki/What_is_Functional_Decision_Theory%3F) | 0.82 |
| [What is AI Safety?](https://stampy.ai/wiki/What_is_AI_Safety%3F) | [Why is safety important for smarter-than-human AI?](https://stampy.ai/wiki/Why_is_safety_important_for_smarter-than-human_AI%3F) | 0.82 |
| [What is Evidential Decision Theory?](https://stampy.ai/wiki/What_is_Evidential_Decision_Theory%3F) | [What is Causal Decision Theory?](https://stampy.ai/wiki/What_is_Causal_Decision_Theory%3F) | 0.82 |
| [What is a follow-up question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_follow-up_question_on_Stampy%27s_Wiki%3F) | [What is a duplicate question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_duplicate_question_on_Stampy%27s_Wiki%3F) | 0.81 |
| [What about AI concerns other than existential safety?](https://stampy.ai/wiki/What_about_AI_concerns_other_than_existential_safety%3F) | [What are some problems in philosophy that are related to AI safety?](https://stampy.ai/wiki/What_are_some_problems_in_philosophy_that_are_related_to_AI_safety%3F) | 0.81 |
| [Might an intelligence explosion never occur?](https://stampy.ai/wiki/Might_an_intelligence_explosion_never_occur%3F) | [How might an intelligence explosion be dangerous?](https://stampy.ai/wiki/How_might_an_intelligence_explosion_be_dangerous%3F) | 0.81 |
| [What are the potential benefits of AI as it grows increasingly sophisticated?](https://stampy.ai/wiki/What_are_the_potential_benefits_of_AI_as_it_grows_increasingly_sophisticated%3F) | [What technological developments could speed up AI progress?](https://stampy.ai/wiki/What_technological_developments_could_speed_up_AI_progress%3F) | 0.81 |
| [What is the intelligence explosion?](https://stampy.ai/wiki/What_is_the_intelligence_explosion%3F) | [How might an intelligence explosion be dangerous?](https://stampy.ai/wiki/How_might_an_intelligence_explosion_be_dangerous%3F) | 0.81 |
| [What is AI alignment?](https://stampy.ai/wiki/What_is_AI_alignment%3F) | [What are some important terms in AI alignment?](https://stampy.ai/wiki/What_are_some_important_terms_in_AI_alignment%3F) | 0.81 |
| [What should be marked as a related question on Stampy's Wiki?](https://stampy.ai/wiki/What_should_be_marked_as_a_related_question_on_Stampy%27s_Wiki%3F) | [What is a follow-up question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_follow-up_question_on_Stampy%27s_Wiki%3F) | 0.81 |
| [Is AI alignment possible?](https://stampy.ai/wiki/Is_AI_alignment_possible%3F) | [Why is AI alignment hard?](https://stampy.ai/wiki/Why_is_AI_alignment_hard%3F) | 0.81 |
| [What would an actually good solution to AI alignment look like?](https://stampy.ai/wiki/What_would_an_actually_good_solution_to_AI_alignment_look_like%3F) | [Are there promising ways to make AI alignment researchers smarter?](https://stampy.ai/wiki/Are_there_promising_ways_to_make_AI_alignment_researchers_smarter%3F) | 0.81 |
| [What is AI alignment?](https://stampy.ai/wiki/What_is_AI_alignment%3F) | [What kind of a challenge is solving AI alignment?](https://stampy.ai/wiki/What_kind_of_a_challenge_is_solving_AI_alignment%3F) | 0.81 |
| [What is the general nature of the concern about AI alignment?](https://stampy.ai/wiki/What_is_the_general_nature_of_the_concern_about_AI_alignment%3F) | [What kind of a challenge is solving AI alignment?](https://stampy.ai/wiki/What_kind_of_a_challenge_is_solving_AI_alignment%3F) | 0.81 |
| [In what ways could weak AI systems help with alignment research?](https://stampy.ai/wiki/In_what_ways_could_weak_AI_systems_help_with_alignment_research%3F) | [Are there promising ways to make AI alignment researchers smarter?](https://stampy.ai/wiki/Are_there_promising_ways_to_make_AI_alignment_researchers_smarter%3F) | 0.81 |
| [What is AI alignment?](https://stampy.ai/wiki/What_is_AI_alignment%3F) | [What would an actually good solution to AI alignment look like?](https://stampy.ai/wiki/What_would_an_actually_good_solution_to_AI_alignment_look_like%3F) | 0.81 |
| [What is a follow-up question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_follow-up_question_on_Stampy%27s_Wiki%3F) | [What is a canonical version of a question on Stampy's Wiki?](https://stampy.ai/wiki/What_is_a_canonical_version_of_a_question_on_Stampy%27s_Wiki%3F) | 0.81 |
| [What would an actually good solution to AI alignment look like?](https://stampy.ai/wiki/What_would_an_actually_good_solution_to_AI_alignment_look_like%3F) | [What kind of a challenge is solving AI alignment?](https://stampy.ai/wiki/What_kind_of_a_challenge_is_solving_AI_alignment%3F) | 0.81 |
| [Do you need a PhD to work on AI Safety?](https://stampy.ai/wiki/Do_you_need_a_PhD_to_work_on_AI_Safety%3F) | [I'm interested in working on AI Safety. What should I do?](https://stampy.ai/wiki/I%27m_interested_in_working_on_AI_Safety._What_should_I_do%3F) | 0.81 |
| [What approaches are AI alignment organizations working on?](https://stampy.ai/wiki/What_approaches_are_AI_alignment_organizations_working_on%3F) | [Are there promising ways to make AI alignment researchers smarter?](https://stampy.ai/wiki/Are_there_promising_ways_to_make_AI_alignment_researchers_smarter%3F) | 0.81 |
| [Why is AGI dangerous?](https://stampy.ai/wiki/Why_is_AGI_dangerous%3F) | [How might AGI kill people?](https://stampy.ai/wiki/How_might_AGI_kill_people%3F) | 0.80 |
| [How can I support alignment researchers to be more productive?](https://stampy.ai/wiki/How_can_I_support_alignment_researchers_to_be_more_productive%3F) | [Could I contribute by offering coaching to alignment researchers? If so, how would I go about this?](https://stampy.ai/wiki/Could_I_contribute_by_offering_coaching_to_alignment_researchers%3F_If_so,_how_would_I_go_about_this%3F) | 0.80 |
| [What would an actually good solution to AI alignment look like?](https://stampy.ai/wiki/What_would_an_actually_good_solution_to_AI_alignment_look_like%3F) | [What are good resources on AI alignment?](https://stampy.ai/wiki/What_are_good_resources_on_AI_alignment%3F) | 0.80 |
| [What is AI alignment?](https://stampy.ai/wiki/What_is_AI_alignment%3F) | [What are good resources on AI alignment?](https://stampy.ai/wiki/What_are_good_resources_on_AI_alignment%3F) | 0.80 |
| [Why is AI Safety important?](https://stampy.ai/wiki/Why_is_AI_Safety_important%3F) | [Why work on AI safety early?](https://stampy.ai/wiki/Why_work_on_AI_safety_early%3F) | 0.80 |
| [What can we do to contribute to AI safety?](https://stampy.ai/wiki/What_can_we_do_to_contribute_to_AI_safety%3F) | [Why is AI Safety important?](https://stampy.ai/wiki/Why_is_AI_Safety_important%3F) | 0.80 |
| [What would an actually good solution to AI alignment look like?](https://stampy.ai/wiki/What_would_an_actually_good_solution_to_AI_alignment_look_like%3F) | [What is the general nature of the concern about AI alignment?](https://stampy.ai/wiki/What_is_the_general_nature_of_the_concern_about_AI_alignment%3F) | 0.80 |
| [Isn’t it immoral to control and impose our values on AI?](https://stampy.ai/wiki/Isn%E2%80%99t_it_immoral_to_control_and_impose_our_values_on_AI%3F) | [Could we tell the AI to do what's morally right?](https://stampy.ai/wiki/Could_we_tell_the_AI_to_do_what%27s_morally_right%3F) | 0.80 |
| [What is the intelligence explosion?](https://stampy.ai/wiki/What_is_the_intelligence_explosion%3F) | [How likely is an intelligence explosion?](https://stampy.ai/wiki/How_likely_is_an_intelligence_explosion%3F) | 0.80 |
| [Why should I worry about superintelligence?](https://stampy.ai/wiki/Why_should_I_worry_about_superintelligence%3F) | [Why might a superintelligence be dangerous?](https://stampy.ai/wiki/Why_might_a_superintelligence_be_dangerous%3F) | 0.80 |
| [Can’t we just program the superintelligence not to harm us?](https://stampy.ai/wiki/Can%E2%80%99t_we_just_program_the_superintelligence_not_to_harm_us%3F) | [Once we notice that a superintelligence given a specific task is starting to try to take over the world, can’t we turn it off, reprogram it, or otherwise correct the problem?](https://stampy.ai/wiki/Once_we_notice_that_a_superintelligence_given_a_specific_task_is_starting_to_try_to_take_over_the_world,_can%E2%80%99t_we_turn_it_off,_reprogram_it,_or_otherwise_correct_the_problem%3F) | 0.80 |
| [What about AI concerns other than existential safety?](https://stampy.ai/wiki/What_about_AI_concerns_other_than_existential_safety%3F) | [What organizations are working on AI existential safety?](https://stampy.ai/wiki/What_organizations_are_working_on_AI_existential_safety%3F) | 0.80 |
| [I'm interested in working on AI Safety. What should I do?](https://stampy.ai/wiki/I%27m_interested_in_working_on_AI_Safety._What_should_I_do%3F) | [How do I form my own views about AI safety?](https://stampy.ai/wiki/How_do_I_form_my_own_views_about_AI_safety%3F) | 0.80 |
| [What is the general nature of the concern about AI alignment?](https://stampy.ai/wiki/What_is_the_general_nature_of_the_concern_about_AI_alignment%3F) | [Why is AI alignment hard?](https://stampy.ai/wiki/Why_is_AI_alignment_hard%3F) | 0.80 |
| [What will be the first transformative applications of AI?](https://stampy.ai/wiki/What_will_be_the_first_transformative_applications_of_AI%3F) | [What military applications of AI will likely exist?](https://stampy.ai/wiki/What_military_applications_of_AI_will_likely_exist%3F) | 0.80 |
| [I’d like a good introduction to AI alignment. Where can I find one?](https://stampy.ai/wiki/I%E2%80%99d_like_a_good_introduction_to_AI_alignment._Where_can_I_find_one%3F) | [What’s a good AI alignment elevator pitch?](https://stampy.ai/wiki/What%E2%80%99s_a_good_AI_alignment_elevator_pitch%3F) | 0.80 |
| [When will transformative AI be created?](https://stampy.ai/wiki/When_will_transformative_AI_be_created%3F) | [What will be the first transformative applications of AI?](https://stampy.ai/wiki/What_will_be_the_first_transformative_applications_of_AI%3F) | 0.80 |
| [What approaches are AI alignment organizations working on?](https://stampy.ai/wiki/What_approaches_are_AI_alignment_organizations_working_on%3F) | [Are there AI alignment projects which governments could usefully put a very large amount of resources into?](https://stampy.ai/wiki/Are_there_AI_alignment_projects_which_governments_could_usefully_put_a_very_large_amount_of_resources_into%3F) | 0.80 |
| [Might an intelligence explosion never occur?](https://stampy.ai/wiki/Might_an_intelligence_explosion_never_occur%3F) | [How could an intelligence explosion be useful?](https://stampy.ai/wiki/How_could_an_intelligence_explosion_be_useful%3F) | 0.79 |
| [What’s a good AI alignment elevator pitch?](https://stampy.ai/wiki/What%E2%80%99s_a_good_AI_alignment_elevator_pitch%3F) | [What are good resources on AI alignment?](https://stampy.ai/wiki/What_are_good_resources_on_AI_alignment%3F) | 0.79 |
| [What approaches are AI alignment organizations working on?](https://stampy.ai/wiki/What_approaches_are_AI_alignment_organizations_working_on%3F) | [What are good resources on AI alignment?](https://stampy.ai/wiki/What_are_good_resources_on_AI_alignment%3F) | 0.79 |
| [If AGI comes from a new paradigm, how likely is it to arise late in the paradigm when it is already deployed at scale versus early when a few people are exploring the idea?](https://stampy.ai/wiki/If_AGI_comes_from_a_new_paradigm,_how_likely_is_it_to_arise_late_in_the_paradigm_when_it_is_already_deployed_at_scale_versus_early_when_a_few_people_are_exploring_the_idea%3F) | [How likely is it that AGI is first developed by a large established org, versus a small startup-y org, versus an academic group, versus a government?](https://stampy.ai/wiki/How_likely_is_it_that_AGI_is_first_developed_by_a_large_established_org,_versus_a_small_startup-y_org,_versus_an_academic_group,_versus_a_government%3F) | 0.79 |
| [What are the potential benefits of AI as it grows increasingly sophisticated?](https://stampy.ai/wiki/What_are_the_potential_benefits_of_AI_as_it_grows_increasingly_sophisticated%3F) | [What will be the first transformative applications of AI?](https://stampy.ai/wiki/What_will_be_the_first_transformative_applications_of_AI%3F) | 0.79 |
| [Why is AI Safety important?](https://stampy.ai/wiki/Why_is_AI_Safety_important%3F) | [What are some problems in philosophy that are related to AI safety?](https://stampy.ai/wiki/What_are_some_problems_in_philosophy_that_are_related_to_AI_safety%3F) | 0.79 |
| [Would AI alignment be hard with deep learning?](https://stampy.ai/wiki/Would_AI_alignment_be_hard_with_deep_learning%3F) | [Is AI alignment possible?](https://stampy.ai/wiki/Is_AI_alignment_possible%3F) | 0.79 |
| [Why might people try to build AGI rather than stronger and stronger narrow AIs?](https://stampy.ai/wiki/Why_might_people_try_to_build_AGI_rather_than_stronger_and_stronger_narrow_AIs%3F) | [Wouldn't it be safer to only build narrow AIs?](https://stampy.ai/wiki/Wouldn%27t_it_be_safer_to_only_build_narrow_AIs%3F) | 0.79 |
| [Isn’t AI just a tool like any other? Won’t AI just do what we tell it to do?](https://stampy.ai/wiki/Isn%E2%80%99t_AI_just_a_tool_like_any_other%3F_Won%E2%80%99t_AI_just_do_what_we_tell_it_to_do%3F) | [Can we just tell an AI to do what we want?](https://stampy.ai/wiki/Can_we_just_tell_an_AI_to_do_what_we_want%3F) | 0.79 |
| [What approaches are AI alignment organizations working on?](https://stampy.ai/wiki/What_approaches_are_AI_alignment_organizations_working_on%3F) | [What kind of a challenge is solving AI alignment?](https://stampy.ai/wiki/What_kind_of_a_challenge_is_solving_AI_alignment%3F) | 0.79 |
| [What are some important terms in AI alignment?](https://stampy.ai/wiki/What_are_some_important_terms_in_AI_alignment%3F) | [What are good resources on AI alignment?](https://stampy.ai/wiki/What_are_good_resources_on_AI_alignment%3F) | 0.79 |
